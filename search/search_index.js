var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Docs","text":"<p>This site contains the project documentation and serves as your starting point for working with the <code>TAInstCalorimetry</code> packaga made available  on PyPI.</p>"},{"location":"index.html#table-of-contents","title":"Table Of Contents","text":"<p>The documentation consists of the following parts:</p> <ul> <li>How to Plot</li> <li>How to do a Tian Correction</li> <li>How to calculate characteristic values</li> <li>An Older How-To Guide</li> <li>Reference</li> </ul> Warning <p><code>CaloCem</code> has been developed without involvement of TA Instruments and is thus independent from the company  and its software.</p>"},{"location":"how-to-guide.html","title":"How-To Guides","text":""},{"location":"how-to-guide.html#interfacing-with-experimental-results-file-from-tam-air-calorimeters-made-easy","title":"Interfacing with experimental results file from TAM Air calorimeters made easy.","text":"<p>After collecting multiple experimental results files from a TAM Air calorimeter you will be left with multiple .xls-files obtained as exports from the device control software. To achieve a side by side comparison of theses results and some basic extraction of relevant parameters, TAInstCalorimetry is here to get this done smoothly.</p> <p>Note: TAInstCalorimetry has been developed without involvement of TA Instruments and is thus independent from the company and its software.</p>"},{"location":"how-to-guide.html#info-downloads","title":"Info / Downloads","text":""},{"location":"how-to-guide.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Example Usage</li> <li>Basic plotting</li> <li>Getting cumulated heat values</li> <li>Identifying peaks</li> <li>Identifying peak onsets</li> <li>Plotting by Category</li> <li>Installation</li> <li>Contributing</li> </ul>"},{"location":"how-to-guide.html#example-usage","title":"Example Usage","text":"<p>Import the <code>tacalorimetry</code> module from TAInstCalorimetry.</p> <pre><code># import\nimport os\nfrom CaloCem import tacalorimetry\n</code></pre> <p>Next, we define where the exported files are stored. With this information at hand, a <code>Measurement</code> is initialized. Experimental raw data and the metadata passed in the course of the measurement are retrieved by the methods <code>get_data()</code> and <code>get_information()</code>, respectively.</p> <pre><code># define data path\n# \"mycalodata\" is the subfoldername where the calorimetry\n# data files (both .csv or .xlsx) are stored\n\npathname = os.path.dirname(os.path.realpath(__file__))\npath_to_data = pathname + os.sep + \"mycalodata\"\n\n# Example: if projectfile is at \"C:\\Users\\myname\\myproject\\myproject.py\", then \"mydata\"\n# refers to \"C:\\Users\\myname\\myproject\\mycalodata\" where the data is stored\n\n# load experiments via class, i.e. instantiate tacalorimetry object with data\ntam = tacalorimetry.Measurement(folder=path_to_data)\n\n# get sample and information\ndata = tam.get_data()\ninfo = tam.get_information()\n</code></pre>"},{"location":"how-to-guide.html#basic-plotting","title":"Basic plotting","text":"<p>Furthermore, the <code>Measurement</code> features a <code>plot()</code>-method for readily visualizing the collected results.</p> <pre><code># make plot\ntam.plot()\n# show plot\ntacalorimetry.plt.show()\n</code></pre> <p>Without further options specified, the <code>plot()</code>-method yields the following.</p> <p></p> <p>The <code>plot()</code>-method can also be tuned to show the temporal course of normalized heat. On the one hand, this \"tuning\" refers to the specification of further keyword arguments such as <code>t_unit</code> and <code>y</code>. On the other hand, the <code>plot()</code>-method returns an object of type <code>matplotlib.axes._subplots.AxesSubplot</code>, which can be used to further customize the plot. In the following, a guide-to-the-eye line is introduced next to adjuting the axes limts, which is not provided for via the <code>plot()</code>-method's signature.</p> <p><pre><code># show cumulated heat plot\nax = tam.plot(\n    t_unit=\"h\",\n    y='normalized_heat',\n    y_unit_milli=False\n)\n\n# define target time\ntarget_h = 1.5\n\n# guide to the eye line\nax.axvline(target_h, color=\"gray\", alpha=0.5, linestyle=\":\")\n\n# set upper limits\nax.set_ylim(top=250)\nax.set_xlim(right=6)\n# show plot\ntacalorimetry.plt.show()\n</code></pre> The following plot is obtained:</p> <p></p>"},{"location":"how-to-guide.html#getting-cumulated-heat-values","title":"Getting cumulated heat values","text":"<p>The cumulated heat after a certain period of time <code>target_h</code> from starting the measurement is a relevant quantity for answering different types of questions. For this purpose, the method <code>get_cumulated_heat_at_hours</code> returns an overview of this parameter for all the samples in the specified folder.</p> <pre><code># get table of cumulated heat at certain age\ncumulated_heats = tam.get_cumulated_heat_at_hours(\n          target_h=target_h,\n          cutoff_min=10\n          )\n\n# show result\nprint(cumulated_heats)\n</code></pre> <p>The return value of the method, <code>cumulated_heats</code> is a <code>pd.DataFrame</code>.</p>"},{"location":"how-to-guide.html#identifying-peaks","title":"Identifying peaks","text":"<p>Next to cumulated heat values detected after a certain time frame from starting the reaction, peaks characteristics can be obtained from the experimental data via the <code>get_peaks</code>-method.</p> <pre><code># get peaks\npeaks = tam.get_peaks(\n    show_plot=True,\n    prominence=0.00001,  # \"sensitivity of peak picking\"\n    cutoff_min=60,  # how much to discard at the beginning of the measurement\n    plt_right_s=4e5,\n    plt_top=1e-2,\n    regex=\".*_\\d\"  # filter samples\n    )\n</code></pre> <p>Tweaking some of the available keyword arguments, the following plot is obtained:</p> <p></p> <p>Please keep in mind, that in particular for samples of ordinary Portland cement (OPC) a clear and unambiguous identification/assigment of peaks remains a challenging task which cannot be achieved in each and every case by TAInstCalorimetry. It is left to the user draw meaningful scientific conclusions from the characteristics derived from this method.</p>"},{"location":"how-to-guide.html#identifying-peak-onsets","title":"Identifying peak onsets","text":"<p>Similarly, the peak onset characteristics are accessible via the <code>get_peak_onsets</code>-method. The resulting plot is shown below.</p> <p><pre><code># get onsets\nonsets = tam.get_peak_onsets(\n    gradient_threshold=0.000001,\n    rolling=10,\n    exclude_discarded_time=True,\n    show_plot=True,\n    regex=\"OPC\"\n)\n</code></pre> </p>"},{"location":"how-to-guide.html#plotting-by-category","title":"Plotting by Category","text":"<p>For introducing the idea of plotting calorimetry data \"by category\" another set of experimental data will be introduced. Next to the calorimetry data alone, information on investigated samples is supplied via an additional source file. In the present example via the file <code>mini_metadata.csv</code>.</p> <p>To begin with, a <code>TAInstCalorimetry.tacalorimetry.Measurement</code>-object is initialized for selected files from the specified <code>`path</code>.</p> <pre><code>import pathlib\nfrom CaloCem import tacalorimetry\n\n# path to experimental calorimetry files\npath = pathlib.Path().cwd().parent / \"CaloCem\" / \"DATA\"\n\n# initialize CaloCem.tacalorimetry.Measurement object\ntam_II = tacalorimetry.Measurement(\n    path, regex=\"myexp.*\", show_info=True, cold_start=True, auto_clean=False\n)\n</code></pre> <p>Next, we need to connect the previously defined object to our metadata provided by the <code>mini_metadata.csv</code>-file. To establish this mapping between experimental results and metadata, the file location, i.e. path, and the column name containing the exact(!) names of the calorimetry files needs to be passed to the <code>add_metadata_source</code>-method. In our case, we declare the column <code>experiment_nr</code> for this purpose</p> <pre><code># add metadata\ntam.add_metadata_source(\"mini_metadata.csv\", \"experiment_nr\")\n</code></pre> <p>Finally, a plotting by category can be carried out by one or multiple categories as shown in the following.</p> <pre><code># define action by one category\ncategorize_by = \"cement_name\"  # 'date', 'cement_amount_g', 'water_amount_g'\n\n# # define action by two or more categories\ncategorize_by = [\"date\", \"cement_name\"]\n\n# loop through plots via generator\nfor this_plot in tam.plot_by_category(categorize_by):\n    # extract parts obtained from generator\n    category_value, ax = this_plot\n    # fine tuning of plot/cosmetics\n    ax.set_ylim(0, 3)\n    # show plot\n    tacalorimetry.plt.show()\n</code></pre> <p>This yields plots of the following kind.</p> <p></p> <p></p>"},{"location":"how-to-guide.html#installation","title":"Installation","text":"<p>Use the package manager pip to install TAInstCalorimetry.</p> <pre><code>pip install CaloCem\n</code></pre>"},{"location":"how-to-guide.html#contributing","title":"Contributing","text":"<p>Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.</p> <p>Please make sure to update tests as appropriate.</p> <p>List of contributors - mj-hofmann - tgaedt</p>"},{"location":"how-to-guide.html#license","title":"License","text":"<p>GNU GPLv3</p>"},{"location":"how-to-guide.html#test","title":"Test","text":""},{"location":"how-to-guide.html#code-styling","title":"Code Styling","text":""},{"location":"plotting.html","title":"Plotting","text":"<p>There are many different use cases for isothermal calorimetry. Here, we focus on the application of isothermal heat flow calorimetry for the hydration of cementitious materials.</p>"},{"location":"plotting.html#basic-plotting-of-calorimetry-data","title":"Basic Plotting of Calorimetry Data","text":"<p>Assume that your calorimetry data is found inside a folder called <code>calo_data</code> and your Python script <code>myscript.py</code>is in the working directory. <pre><code>.\n\u251c\u2500\u2500 myscript.py\n\u2514\u2500\u2500 calo_data\n    \u251c\u2500\u2500 calofile1.csv\n    \u2514\u2500\u2500 calofile2.csv\n</code></pre></p> <p>It is very easy to load the calorimetry files and to plot them. The file <code>myscript.py</code> could read like this. First, we create a Path object <code>datapath</code> using the pathlib package that is directed at the folder which contains the raw instrument data (<code>calo_data</code> in this example). The advantage of using the <code>pathlib</code> package is that we do not have to worry if the user of our code is running Linux, MacOS, or Windows. The <code>Path()</code> object ensures that the path definition always works. In our example, <code>Path(__file__).parent</code> contains the absolute path to the folder in which the script (here <code>myscript.py</code>) is located, independent of the operating system. By writing <code>Path(__file__).parent / \"calo_data\"</code> we create a <code>Path()</code> object which contains the absolute path to <code>calo_data</code>.</p> <p>After we have obtained the path, we pass it to <code>ta.Measurement()</code>. Besides the <code>Path</code> object, we can pass further arguments such as the option <code>show_info=True</code> which prints the names of the calo files being loaded in the terminal.</p> <p><pre><code>from CaloCem import tacalorimetry as ta\nfrom pathlib import Path\n\ndatapath = Path(__file__).parent / \"calo_data\"\n\n# create the calorimetry Measurement object\ntam = ta.Measurement(\n    folder=datapath,\n    show_info=True,\n    auto_clean=False,\n)\n\n# plot the data\ntam.plot()\n</code></pre> This would yield something like the following plot:</p> <p></p> <p>The plot has at least three issues:</p> <ul> <li>the y-axis and the x-axis are automatically scaled to include the maximumum values</li> <li>the legend is not visible</li> <li>by default the plot method plots the normalized heat flow in mW/g, maybe another parameters is desired</li> </ul>"},{"location":"plotting.html#customizing-the-plot","title":"Customizing the plot","text":""},{"location":"plotting.html#choosing-different-variables-for-the-y-axis","title":"Choosing different variables for the y-axis","text":"<p>If only a different y-axis variable is desired, this can simply be achieved by defining the name of the desired parameter:</p> <pre><code>tam.plot(y=\"heat_j_g\")\n</code></pre>"},{"location":"plotting.html#full-customization","title":"Full customization","text":"<p>The <code>plot()</code> method returns a Matplotlib axes object.  Therefore, we can manipulate the plot as normal, e.g., by defining the limits of both axes or by defining the location of the legend (as shown in the code below).</p> <pre><code>ax = tam.plot(\n    y=\"normalized_heat_flow_w_g\",\n    t_unit=\"h\",  # time axis in hours\n    y_unit_milli=True,\n)\n\n# set upper limits\nax.set_ylim(0, 6)\nax.set_xlim(0, 48)\nax.legend(bbox_to_anchor=(1., 1), loc=\"upper right\")\n</code></pre>"},{"location":"plotting.html#plotting-heat-flow-and-heat-in-subplots","title":"Plotting Heat Flow and Heat in Subplots","text":"<p>Often, both the initial phase of hydration is of interest and also both the heat flow and the heat are relevant.  Here is code which allows plotting such data to a neat 2x2 grid.</p> <pre><code>plot_configs = [\n    {\"ycol\": \"normalized_heat_flow_w_g\", \"xlim\": 1, \"ylim\": 0.05},\n    {\"ycol\": \"normalized_heat_flow_w_g\", \"xlim\": 48, \"ylim\": 0.005},\n    {\"ycol\": \"normalized_heat_j_g\", \"xlim\": 1, \"ylim\": 30},\n    {\"ycol\": \"normalized_heat_j_g\", \"xlim\": 48, \"ylim\": 300},\n]\n\nfig, axs = ta.plt.subplots(2, 2, layout=\"constrained\")\nfor ax, config in zip(axs.flatten(), plot_configs):\n    tam.plot(y=config[\"ycol\"], t_unit=\"h\", y_unit_milli=False, ax=ax)\n    ax.set_xlim(0, config[\"xlim\"])\n    ax.set_ylim(0, config[\"ylim\"])\n    ax.get_legend().remove()\nta.plt.show()\n</code></pre> <p></p>"},{"location":"quantification.html","title":"Quantification of Calorimetry Data","text":""},{"location":"quantification.html#peak-detection","title":"Peak Detection","text":"<p>First we load the data</p> <p><pre><code># %%\nfrom pathlib import Path\nimport CaloCem.tacalorimetry as ta\n\n# define the Path of the folder with the calorimetry data\ndatapath = Path()\n\n# experiments via class\ntam = ta.Measurement(\n    folder=datapath,\n    regex=r\".*file.*\",\n    show_info=True,\n    auto_clean=False,\n    cold_start=True,\n)\n</code></pre> Then, before we apply the <code>get_peaks()</code> method, we can define the ProcessingParameters if the default options are not suitable. Here we define the peak prominence and give it the value 1e-4. Note that only the largest peak will be detected if the prominence is set to 1e-3 in this example.</p> <p><pre><code># define the processing parameters\nprocessparams = ta.ProcessingParameters()\nprocessparams.peakdetection.prominence = 1e-4\n\n# plot the peak position\nfig, ax = ta.plt.subplots()\n\n# get peaks (returns both a dataframe and extends the axes object)\npeaks_found = tam.get_peaks(processparams, plt_right_s=3e5, ax=ax, show_plot=True)\nax.set_xlim(0, 100000)\nta.plt.show()\n</code></pre> </p> <p><code>peaks_found</code> is a tuple.  The first element contains the Dataframe with the parameters of the detected peaks. It could be exported to a csv file via</p> <pre><code>df = peaks_found[0].iloc[:,[0,5,6,9]]\ndf.to_csv(plotpath / \"example_get_peaks.csv\", index=False)\n</code></pre> <p>The dataframe looks like this:</p> time_s normalized_heat_flow_w_g normalized_heat_j_g prominence 29565.2 0.00207652 75.7002 0.00109303 82364.8 0.00164736 162.984 0.000207758"},{"location":"quantification.html#maximum-slope-of-c3s-reaction-detection","title":"Maximum Slope (of C3S Reaction) Detection","text":"<p>Programmatic, automatic detection the maximum slope can be a little tricky. The first example is straightforward and looks a normal Portland cement hydration case. Assuming that the data is already loaded, we can inistantiate the ProcessingParameters object. The algorithm detects the maxima of the gradient of the heat flow. It is therefore very useful to apply a little smoothing to the first derivative.</p> <p><pre><code>processparams = ta.ProcessingParameters()\nprocessparams.spline_interpolation.apply = True\nprocessparams.spline_interpolation.smoothing_1st_deriv = 1e-12\n\n# get peak onsets via alternative method\nfig, ax = ta.plt.subplots()\nonsets_spline = tam.get_maximum_slope(\n    processparams=processparams,\n    show_plot=True,\n    ax = ax\n)\n</code></pre> If we set the parameter <code>show_plot</code> to <code>True</code>, we get a nice visual feedback on the gradient and the detected maximum slope (as a green line). The gradient is multiplied by a factor of 10.000 and shifted upwards.</p> <p></p>"},{"location":"reference.html","title":"Reference","text":"<p>This part of the project documentation shows the technical implementation  of the <code>CaloCem</code> project code.</p>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement","title":"<code>Measurement</code>","text":"<p>A base class for handling and processing isothermal heat flow calorimetry data.</p> <p>Currently supported file formats are .xls and .csv files. Only TA Instruments data files are supported at the moment.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>path to folder containing .xls and/or .csv experimental result files. The default is None.</p> <code>None</code> <code>show_info</code> <code>bool</code> <p>whether or not to print some informative lines during code execution. The default is True.</p> <code>True</code> <code>regex</code> <code>str</code> <p>regex pattern to include only certain experimental result files during initialization. The default is None.</p> <code>None</code> <code>auto_clean</code> <code>bool</code> <p>whether or not to exclude NaN values contained in the original files and combine data from differently names temperature columns. The default is False.</p> <code>False</code> <code>cold_start</code> <code>bool</code> <p>whether or not to use \"pickled\" files for initialization; save time on reading</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import CaloCem as ta\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt;\n&gt;&gt;&gt; calodatapath = Path(__file__).parent\n&gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n</code></pre> <p>We can use a regex pattern to only include certain files in the datafolder. Here we assume that we only want to load .csv files which contain the string \"bm\".</p> <pre><code>&gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, regex=r\".*bm.*.csv\", show_info=True)\n</code></pre> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>class Measurement:\n    \"\"\"\n    A base class for handling and processing isothermal heat flow calorimetry data.\n\n    Currently supported file formats are .xls and .csv files.\n    Only TA Instruments data files are supported at the moment.\n\n    Parameters\n    ----------\n    folder : str, optional\n        path to folder containing .xls and/or .csv experimental result\n        files. The default is None.\n    show_info : bool, optional\n        whether or not to print some informative lines during code\n        execution. The default is True.\n    regex : str, optional\n        regex pattern to include only certain experimental result files\n        during initialization. The default is None.\n    auto_clean : bool, optional\n        whether or not to exclude NaN values contained in the original\n        files and combine data from differently names temperature columns.\n        The default is False.\n    cold_start : bool, optional\n        whether or not to use \"pickled\" files for initialization; save time\n        on reading\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import CaloCem as ta\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; calodatapath = Path(__file__).parent\n    &gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n\n    We can use a regex pattern to only include certain files in the datafolder. Here we assume that we only want to load .csv files which contain the string \"bm\".\n\n    &gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, regex=r\".*bm.*.csv\", show_info=True)\n\n    \"\"\"\n\n    # init\n    _info = pd.DataFrame()\n    _data = pd.DataFrame()\n    _data_unprocessed = (\n        pd.DataFrame()\n    )  # helper to store experimental data as loaded from files\n\n    # further metadata\n    _metadata = pd.DataFrame()\n    _metadata_id = \"\"\n\n    # define pickle filenames\n    _file_data_pickle = pathlib.Path().cwd() / \"_data.pickle\"\n    _file_info_pickle = pathlib.Path().cwd() / \"_info.pickle\"\n\n    #\n    # init\n    #\n    def __init__(\n        self,\n        folder=None,\n        show_info=True,\n        regex=None,\n        auto_clean=False,\n        cold_start=True,\n        processparams=None,\n    ):\n        \"\"\"\n        intialize measurements from folder\n\n\n        \"\"\"\n\n        if not isinstance(processparams, ProcessingParameters):\n            self.processparams = ProcessingParameters()\n        else:\n            self.processparams = processparams\n\n        # read\n        if folder:\n            if cold_start:\n                # get data and parameters\n                self._get_data_and_parameters_from_folder(\n                    folder, regex=regex, show_info=show_info\n                )\n            else:\n                # get data and parameters from pickled files\n                self._get_data_and_parameters_from_pickle()\n            try:\n                if auto_clean:\n                    # remove NaN values and merge time columns\n                    self._auto_clean_data()\n            except Exception as e:\n                # info\n                print(e)\n                raise AutoCleanException\n                # return\n                return\n\n        if self.processparams.downsample.apply:\n            self._apply_adaptive_downsampling()\n        # Message\n        print(\n            \"================\\nAre you missing some samples? Try rerunning with auto_clean=True and cold_start=True.\\n=================\"\n        )\n\n    #\n    # get_data_and_parameters_from_folder\n    #\n    def _get_data_and_parameters_from_folder(self, folder, regex=None, show_info=True):\n        \"\"\"\n        get_data_and_parameters_from_folder\n        \"\"\"\n\n        if not isinstance(folder, str):\n            # convert\n            folder = str(folder)\n\n        # loop folder\n        for f in os.listdir(folder):\n            if not f.endswith((\".xls\", \".csv\")):\n                # go to next\n                continue\n\n            if regex:\n                # check match\n                if not re.match(regex, f):\n                    # skip this file\n                    continue\n\n            # info\n            if show_info:\n                print(f\"Reading {f}.\")\n\n            # define file\n            file = folder + os.sep + f\n\n            # check xls\n            if f.endswith(\".xls\"):\n                # collect information\n                try:\n                    self._info = pd.concat(\n                        [\n                            self._info,\n                            self._read_calo_info_xls(file, show_info=show_info),\n                        ]\n                    )\n                except Exception:\n                    # initialize\n                    if self._info.empty:\n                        self._info = self._read_calo_info_xls(file, show_info=show_info)\n\n                # collect data\n                try:\n                    self._data = pd.concat(\n                        [\n                            self._data,\n                            self._read_calo_data_xls(file, show_info=show_info),\n                        ]\n                    )\n\n                except Exception:\n                    # initialize\n                    if self._data.empty:\n                        self._data = self._read_calo_data_xls(file, show_info=show_info)\n\n            # append csv\n            if f.endswith(\".csv\"):\n                # collect data\n                try:\n                    self._data = pd.concat(\n                        [\n                            self._data,\n                            self._read_calo_data_csv(file, show_info=show_info),\n                        ]\n                    )\n\n                except Exception:\n                    # initialize\n                    if self._data.empty:\n                        self._data = self._read_calo_data_csv(file, show_info=show_info)\n\n                # collect information\n                try:\n                    self._info = pd.concat(\n                        [\n                            self._info,\n                            self._read_calo_info_csv(file, show_info=show_info),\n                        ]\n                    )\n                except Exception:\n                    # initialize\n                    if self._info.empty:\n                        try:\n                            self._info = self._read_calo_info_csv(\n                                file, show_info=show_info\n                            )\n                        except Exception:\n                            pass\n\n        # get \"heat_j\" columns if the column is not part of the source files\n        try:\n            self._infer_heat_j_column()\n        except Exception:\n            pass\n\n        # if self.processparams.downsample.apply is not False:\n        #     self._apply_adaptive_downsampling()\n        # write _data and _info to pickle\n        with open(self._file_data_pickle, \"wb\") as f:\n            pickle.dump(self._data, f)\n        with open(self._file_info_pickle, \"wb\") as f:\n            pickle.dump(self._info, f)\n\n        # store experimental data for recreating state after reading from files\n        self._data_unprocessed = self._data.copy()\n\n    #\n    # get data and information from pickled files\n    #\n    def _get_data_and_parameters_from_pickle(self):\n        \"\"\"\n        get data and information from pickled files\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # read from pickle\n        try:\n            self._data = pd.read_pickle(self._file_data_pickle)\n            self._info = pd.read_pickle(self._file_info_pickle)\n            # store experimental data for recreating state after reading from files\n            self._data_unprocessed = self._data.copy()\n        except FileNotFoundError:\n            # raise custom Exception\n            raise ColdStartException()\n\n        # log\n        logging.info(\"_data and _info loaded from pickle files.\")\n\n    #\n    # determine csv data range\n    #\n    def _determine_data_range_csv(self, file):\n        \"\"\"\n        determine csv data range of CSV-file.\n\n        Parameters\n        ----------\n        file : str\n            filepath.\n\n        Returns\n        -------\n        empty_lines : TYPE\n            DESCRIPTION.\n\n        \"\"\"\n        # open csv file\n        thefile = open(file)\n        # detect empty lines which are characteristic at the beginning and\n        # end of the data block\n        empty_lines = [\n            index for index, line in enumerate(csv.reader(thefile)) if len(line) == 0\n        ]\n        # nr_lines = empty_lines[1] - empty_lines[0] - 2\n        return empty_lines\n\n    #\n    # read csv data\n    #\n    def _read_calo_data_csv(self, file, show_info=True):\n        \"\"\"\n        try reading calorimetry data from csv file via multiple options\n\n        Parameters\n        ----------\n        file : str | pathlib.Path\n            path to csv fileto be read.\n        show_info : bool, optional\n            flag whether or not to show information. The default is True.\n\n        Returns\n        -------\n        pd.DataFrame\n\n        \"\"\"\n\n        try:\n            data = self._read_calo_data_csv_comma_sep(file, show_info=show_info)\n        except Exception:\n            data = self._read_calo_data_csv_tab_sep(file, show_info=show_info)\n\n        # valid read\n        if data is None:\n            # log\n            logging.info(f\"\\u2716 reading {file} FAILED.\")\n\n        # log\n        logging.info(f\"\\u2714 reading {file} successful.\")\n\n        # return\n        return data\n\n    #\n    # read csv data\n    #\n    def _read_calo_data_csv_comma_sep(self, file, show_info=True):\n        \"\"\"\n        read data from csv file\n\n        Parameters\n        ----------\n        file : str\n            filepath.\n\n        Returns\n        -------\n        data : pd.DataFrame\n            experimental data contained in file.\n\n        \"\"\"\n\n        # define Excel file\n        data = pd.read_csv(\n            file, header=None, sep=\"No meaningful separator\", engine=\"python\"\n        )\n\n        # check for tab-separation\n        if \"\\t\" in data.at[0, 0]:\n            # raise Exception\n            raise ValueError\n\n        # look for potential index indicating in-situ-file\n        if data[0].str.contains(\"Reaction start\").any():\n            # get target row\n            helper = data[0].str.contains(\"Reaction start\")\n            # get row\n            start_row = helper[helper].index.tolist()[0]\n            # get offset for in-situ files\n            t_offset_in_situ_s = float(data.at[start_row, 0].split(\",\")[0])\n\n        data = utils.parse_rowwise_data(data)\n        data = utils.tidy_colnames(data)\n\n        data = utils.remove_unnecessary_data(data)\n\n        # type conversion\n        data = utils.convert_df_to_float(data)\n\n        # check for \"in-situ\" sample --&gt; reset\n        try:\n            # offset\n            data[\"time_s\"] -= t_offset_in_situ_s\n            # write to log\n            logging.info(\n                f\"\\u26a0 Consider {file} as in-situ-file --&gt; time-scale adjusted.\"\n            )\n        except Exception:\n            pass\n\n        # restrict to \"time_s\" &gt; 0\n        data = data.query(\"time_s &gt; 0\").reset_index(drop=True)\n\n        # add sample information\n        data = utils.add_sample_info(data, file)\n\n\n        # if self.processparams.downsample.apply:\n        #     data = self._apply_adaptive_downsampling(data)\n\n        # return\n        return data\n\n    #\n    # read csv data\n    #\n    def _read_calo_data_csv_tab_sep(self, file: str, show_info=True) -&gt; pd.DataFrame:\n        \"\"\"\n        Parameters\n        ----------\n        file : str | pathlib.Path\n            path to tab separated csv-files from \"older\" versions of the device.\n        show_info : bool, optional\n            flag whether or not to show information. The default is True.\n\n        Returns\n        -------\n        pd.DataFrame\n\n        \"\"\"\n\n        # read\n        raw = pd.read_csv(file, sep=\"\\t\", header=None)\n\n        # process\n        data = raw.copy()\n\n        # get sample mass (if available)\n        try:\n            # get mass, first row in 3rd column is the title\n            # the assumption is that the sample weight is one value on top of the 3rd column\n            mass_index = raw.index[raw.iloc[:, 3].notna()]\n            mass = float(raw.iloc[mass_index[1], 3])\n        except IndexError:\n            # set mass to None\n            mass = None\n            # go on\n            pass\n\n        # get \"reaction start\" time (if available)\n        try:\n            # get \"reaction start\" time in seconds\n            _helper = data[data.iloc[:, 2].str.lower() == \"reaction start\"].head(1)\n            # convert to float\n            t0 = float(_helper[0].values[0])\n        except Exception:\n            # set t0 to None\n            t0 = None\n            # go on\n            pass\n\n        # remove all-Nan columns\n        data = data.dropna(how=\"all\", axis=1)\n\n        # restrict to first two columns\n        data = data.iloc[:, :2]\n\n        # rename\n        try:\n            data.columns = [\"time_s\", \"heat_flow_mw\"]\n        except ValueError:\n            # return empty DataFrame\n            return pd.DataFrame({\"time_s\": 0}, index=[0])\n\n        # get data columns\n        data = data.loc[3:, :].reset_index(drop=True)\n\n        # convert data types\n        data[\"time_s\"] = data[\"time_s\"].astype(float)\n        data[\"heat_flow_mw\"] = data[\"heat_flow_mw\"].apply(\n            lambda x: float(x.replace(\",\", \".\"))\n        )\n\n        # convert to same unit\n        data[\"heat_flow_w\"] = data[\"heat_flow_mw\"] / 1000\n\n        # calculate cumulative heat flow\n        data[\"heat_j\"] = integrate.cumulative_trapezoid(\n            data[\"heat_flow_w\"], x=data[\"time_s\"], initial=0\n        )\n\n        # remove \"heat_flow_w\" column\n        del data[\"heat_flow_mw\"]\n\n        # take into account time offset via \"reactin start\" time\n        if t0:\n            data[\"time_s\"] -= t0\n\n        # calculate normalized heat flow and heat\n        if mass:\n            data[\"normalized_heat_flow_w_g\"] = data[\"heat_flow_w\"] / mass\n            data[\"normalized_heat_j_g\"] = data[\"heat_j\"] / mass\n\n        # restrict to \"time_s\" &gt; 0\n        data = data.query(\"time_s &gt;= 0\").reset_index(drop=True)\n\n        # add sample information\n        data[\"sample\"] = file\n        data[\"sample_short\"] = pathlib.Path(file).stem\n\n        # type conversion\n        data = utils.convert_df_to_float(data)\n\n        # return\n        return data\n\n    #\n    # read csv info\n    #\n    def _read_calo_info_csv(self, file, show_info=True):\n        \"\"\"\n        read info from csv file\n\n        Parameters\n        ----------\n        file : str\n            filepath.\n\n        Returns\n        -------\n        info : pd.DataFrame\n            information (metadata) contained in file\n\n        \"\"\"\n\n        try:\n            # determine number of lines to skip\n            empty_lines = self._determine_data_range_csv(file)\n            # read info block from csv-file\n            info = pd.read_csv(\n                file, nrows=empty_lines[0] - 1, names=[\"parameter\", \"value\"]\n            ).dropna(subset=[\"parameter\"])\n            # the last block is not really meta data but summary data and\n            # somewhat not necessary\n        except IndexError:\n            # return empty DataFrame\n            info = pd.DataFrame()\n\n        # add sample name as column\n        info[\"sample\"] = file\n        info[\"sample_short\"] = pathlib.Path(file).stem\n\n        # return\n        return info\n\n    #\n    # read excel info\n    #\n    def _read_calo_info_xls(self, file, show_info=True):\n        \"\"\"\n        read information from xls-file\n\n        Parameters\n        ----------\n        file : str\n            filepath.\n        show_info : bool, optional\n            flag whether or not to show information. The default is True.\n\n        Returns\n        -------\n        info : pd.DataFrame\n            information (metadata) contained in file\n\n        \"\"\"\n        # specify Excel\n        xl = pd.ExcelFile(file)\n\n        try:\n            # get experiment info (first sheet)\n            df_experiment_info = xl.parse(\n                sheet_name=\"Experiment info\", header=0, names=[\"parameter\", \"value\"]\n            ).dropna(subset=[\"parameter\"])\n            # use first row as header\n            df_experiment_info = df_experiment_info.iloc[1:, :]\n\n            # add sample information\n            df_experiment_info[\"sample\"] = file\n            df_experiment_info[\"sample_short\"] = pathlib.Path(file).stem\n\n            # rename variable\n            info = df_experiment_info\n\n            # return\n            return info\n\n        except Exception as e:\n            if show_info:\n                print(e)\n                print(f\"==&gt; ERROR in file {file}\")\n\n    #\n    # read excel data\n    #\n    def _read_calo_data_xls(self, file, show_info=True):\n        \"\"\"\n        read data from xls-file\n\n        Parameters\n        ----------\n        file : str\n            filepath.\n        show_info : bool, optional\n            flag whether or not to show information. The default is True.\n\n        Returns\n        -------\n        data : pd.DataFrame\n            data contained in file\n\n        \"\"\"\n\n        # define Excel file\n        xl = pd.ExcelFile(file)\n\n        try:\n            # parse \"data\" sheet\n            df_data = xl.parse(\"Raw data\", header=None)\n\n            # replace init timestamp\n            df_data.iloc[0, 0] = \"time\"\n\n            # get new column names\n            new_columnames = []\n            for i, j in zip(df_data.iloc[0, :], df_data.iloc[1, :]):\n                # build\n                new_columnames.append(\n                    re.sub(r\"[\\s\\n\\[\\]\\(\\)\u00b0 _]+\", \"_\", f\"{i}_{j}\".lower())\n                    .replace(\"/\", \"_\")\n                    .replace(\"_signal_\", \"_\")\n                )\n\n            # set\n            df_data.columns = new_columnames\n\n            # cut out data part\n            df_data = df_data.iloc[2:, :].reset_index(drop=True)\n\n            # drop column\n            try:\n                df_data = df_data.drop(columns=[\"time_markers_nan\"])\n            except KeyError:\n                pass\n\n            # remove columns with too many NaNs\n            df_data = df_data.dropna(axis=1, thresh=3)\n            # # remove rows with NaNs\n            # df_data = df_data.dropna(axis=0)\n\n            # float conversion\n            for _c in df_data.columns:\n                # convert\n                df_data[_c] = df_data[_c].astype(float)\n\n            # add sample information\n            df_data[\"sample\"] = file\n            df_data[\"sample_short\"] = pathlib.Path(file).stem\n\n            # rename\n            data = df_data\n\n            # log\n            logging.info(f\"\\u2714 reading {file} successful.\")\n\n            # return\n            return data\n\n        except Exception as e:\n            if show_info:\n                print(\n                    \"\\n\\n===============================================================\"\n                )\n                print(f\"{e} in file '{pathlib.Path(file).name}'\")\n                print(\"Please, rename the data sheet to 'Raw data' (device default).\")\n                print(\n                    \"===============================================================\\n\\n\"\n                )\n\n            # log\n            logging.info(f\"\\u2716 reading {file} FAILED.\")\n\n            # return\n            return None\n\n    #\n    # iterate samples\n    #\n    def _iter_samples(self, regex=None):\n        \"\"\"\n        iterate samples and return corresponding data\n\n        Returns\n        -------\n        sample (str) : name of the current sample\n        data (pd.DataFrame) : data corresponding to the current sample\n        \"\"\"\n\n        for sample, data in self._data.groupby(by=\"sample\"):\n            if regex:\n                if not re.findall(regex, sample):\n                    continue\n\n            yield sample, data\n\n    #\n    # auto clean data\n    #\n    def _auto_clean_data(self):\n        \"\"\"\n        remove NaN values from self._data and merge differently named columns\n        representing the (constant) temperature set for the measurement\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # remove NaN values and reset index\n        self._data = self._data.dropna(\n            subset=[c for c in self._data.columns if re.match(\"normalized_heat\", c)]\n        ).reset_index(drop=True)\n\n        # determine NaN count\n        nan_count = self._data[\"temperature_temperature_c\"].isna().astype(\n            int\n        ) + self._data[\"temperature_c\"].isna().astype(int)\n\n        # consolidate temperature columns\n        if (\n            \"temperature_temperature_c\" in self._data.columns\n            and \"temperature_c\" in self._data.columns\n        ):\n            # use values from column \"temperature_c\" and set the values to column\n            # \"temperature_c\"\n            self._data.loc[\n                (self._data[\"temperature_temperature_c\"].isna()) &amp; (nan_count == 1),\n                \"temperature_temperature_c\",\n            ] = self._data.loc[\n                (~self._data[\"temperature_c\"].isna()) &amp; (nan_count == 1),\n                \"temperature_c\",\n            ]\n\n            # remove values from column \"temperature_c\"\n            self._data = self._data.drop(columns=[\"temperature_c\"])\n\n        # rename column\n        self._data = self._data.rename(\n            columns={\"temperature_temperature_c\": \"temperature_c\"}\n        )\n\n    #\n    # plot\n    #\n    def plot(\n        self,\n        t_unit=\"h\",\n        y=\"normalized_heat_flow_w_g\",\n        y_unit_milli=True,\n        regex=None,\n        show_info=True,\n        ax=None,\n    ):\n        \"\"\"\n\n        Plot the calorimetry data.\n\n        Parameters\n        ----------\n        t_unit : str, optional\n            time unit. The default is \"h\". Options are \"s\", \"min\", \"h\", \"d\".\n        y : str, optional\n            y-axis. The default is \"normalized_heat_flow_w_g\". Options are\n            \"normalized_heat_flow_w_g\", \"heat_flow_w\", \"normalized_heat_j_g\",\n            \"heat_j\".\n        y_unit_milli : bool, optional\n            whether or not to plot y-axis in Milliwatt. The default is True.\n        regex : str, optional\n            regex pattern to include only certain samples during plotting. The\n            default is None.\n        show_info : bool, optional\n            whether or not to show information. The default is True.\n        ax : matplotlib.axes._axes.Axes, optional\n            axis to plot to. The default is None.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import CaloCem as ta\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; calodatapath = Path(__file__).parent\n        &gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n        &gt;&gt;&gt; tam.plot(t_unit=\"h\", y=\"normalized_heat_flow_w_g\", y_unit_milli=False)\n\n        \"\"\"\n\n        # y-value\n        if y == \"normalized_heat_flow_w_g\":\n            y_column = \"normalized_heat_flow_w_g\"\n            y_label = \"Normalized Heat Flow / [W/g]\"\n        elif y == \"heat_flow_w\":\n            y_column = \"heat_flow_w\"\n            y_label = \"Heat Flow / [W]\"\n        elif y == \"normalized_heat_j_g\":\n            y_column = \"normalized_heat_j_g\"\n            y_label = \"Normalized Heat / [J/g]\"\n        elif y == \"heat_j\":\n            y_column = \"heat_j\"\n            y_label = \"Heat / [J]\"\n\n        if y_unit_milli:\n            y_label = y_label.replace(\"[\", \"[m\")\n\n        # x-unit\n        if t_unit == \"s\":\n            x_factor = 1.0\n        elif t_unit == \"min\":\n            x_factor = 1 / 60\n        elif t_unit == \"h\":\n            x_factor = 1 / (60 * 60)\n        elif t_unit == \"d\":\n            x_factor = 1 / (60 * 60 * 24)\n\n        # y-unit\n        if y_unit_milli:\n            y_factor = 1000\n        else:\n            y_factor = 1\n\n        for sample, data in self._iter_samples():\n            if regex:\n                if not re.findall(rf\"{regex}\", os.path.basename(sample)):\n                    continue\n            data[\"time_s\"] = data[\"time_s\"] * x_factor\n            # all columns containing heat\n            heatcols = [s for s in data.columns if \"heat\" in s]\n            data[heatcols] = data[heatcols] * y_factor\n            ax, _ = utils.create_base_plot(data, ax, \"time_s\", y_column, sample)\n            ax = utils.style_base_plot(\n                ax,\n                y_label,\n                t_unit,\n                sample,\n            )\n        return ax\n\n    #\n    # plot by category\n    #\n    def plot_by_category(\n        self, categories, t_unit=\"h\", y=\"normalized_heat_flow_w_g\", y_unit_milli=True\n    ):\n        \"\"\"\n        plot by category, wherein the category is based on the information passed\n        via \"self._add_metadata_source\". Options available as \"category\" are\n        accessible via \"self.get_metadata_grouping_options\"\n\n        Parameters\n        ----------\n        categories : str, list[str]\n            category (from \"self.get_metadata_grouping_options\") to group by.\n            specify a string or a list of strings here\n        t_unit : TYPE, optional\n            see \"self.plot\". The default is \"h\".\n        y : TYPE, optional\n            see \"self.plot\". The default is \"normalized_heat_flow_w_g\".\n        y_unit_milli : TYPE, optional\n            see \"self.plot\". The default is True.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import CaloCem as ta\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; calodatapath = Path(__file__).parent\n        &gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n        &gt;&gt;&gt; tam.plot_by_category(categories=\"sample\")\n\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        def build_helper_string(values: list) -&gt; str:\n            \"\"\"\n            build a \"nicely\" formatted string from a supplied list\n            \"\"\"\n\n            if len(values) == 2:\n                # connect with \"and\"\n                formatted = \" and \".join([str(i) for i in values])\n            elif len(values) &gt; 2:\n                # connect with comma and \"and\" for last element\n                formatted = (\n                    \", \".join([str(i) for i in values[:-1]]) + \" and \" + str(values[-1])\n                )\n            else:\n                formatted = \"---\"\n\n            # return\n            return formatted\n\n        # loop category values\n        for selections, _ in self._metadata.groupby(by=categories):\n            if isinstance(selections, tuple):\n                # - if multiple categories to group by are specified -\n                # init helper DataFrame\n                target_idx = pd.DataFrame()\n                # identify corresponding samples\n                for selection, category in zip(selections, categories):\n                    target_idx[category] = self._metadata[category] == selection\n                # get relevant indices\n                target_idx = target_idx.sum(axis=1) == len(categories)\n                # define title\n                title = f\"Grouped by { build_helper_string(categories)} ({build_helper_string(selections)})\"\n            else:\n                # - if only one(!) category to group by is specified -\n                # identify corresponding samples\n                target_idx = self._metadata[categories] == selections\n                # define title\n                title = f\"Grouped by {categories} ({selections})\"\n\n            # pick relevant samples\n            target_samples = self._metadata.loc[target_idx, self._metadata_id]\n\n            # build corresponding regex\n            regex = \"(\" + \")|(\".join(target_samples) + \")\"\n\n            # plot\n            ax = self.plot(regex=regex, t_unit=t_unit, y=y, y_unit_milli=y_unit_milli)\n\n            # set title\n            ax.set_title(title)\n\n            # yield latest plot\n            yield selections, ax\n\n    @staticmethod\n    def _plot_peak_positions(\n        data, ax, _age_col, _target_col, peaks, sample, plt_top, plt_right_s\n    ):\n        \"\"\"\n        Plot detected peaks.\n        \"\"\"\n\n        ax, new_ax = utils.create_base_plot(data, ax, _age_col, _target_col, sample)\n\n        ax.plot(\n            data[_age_col][peaks],\n            data[_target_col][peaks],\n            \"x\",\n            color=\"red\",\n        )\n\n        ax.vlines(\n            x=data[_age_col][peaks],\n            ymin=0,\n            ymax=data[_target_col][peaks],\n            color=\"red\",\n        )\n\n        limits = {\n            \"left\": ax.get_xlim()[0],\n            \"right\": plt_right_s,\n            \"bottom\": 0,\n            \"top\": plt_top,\n        }\n\n        ax = utils.style_base_plot(ax, _target_col, _age_col, sample, limits)\n\n        if new_ax:\n            plt.show()\n\n    @staticmethod\n    def _plot_maximum_slope(\n        data,\n        ax,\n        age_col,\n        target_col,\n        sample,\n        characteristics,\n        time_discarded_s,\n        save_path=None,\n    ):\n        ax, new_ax = utils.create_base_plot(data, ax, age_col, target_col, sample)\n\n        ax.plot(\n            data[age_col],\n            data[\"gradient\"] * 1e4 + 0.001,\n            label=\"gradient * 1e4 + 1mW\",\n        )\n\n        # add vertical lines\n        for _idx, _row in characteristics.iterrows():\n            # vline\n            ax.axvline(_row.at[age_col], color=\"green\", alpha=0.3)\n\n        limits = {\"left\": 100, \"right\": ax.get_xlim()[1], \"bottom\": 0, \"top\": 0.01}\n\n        ax = utils.style_base_plot(\n            ax, target_col, age_col, sample, limits, time_discarded_s=time_discarded_s\n        )\n\n        ax.set_xscale(\"log\")\n\n        if new_ax:\n            if save_path:\n                sample_name = pathlib.Path(sample).stem\n                plt.savefig(save_path / f\"maximum_slope_detect_{sample_name}.png\")\n            else:\n                plt.show()\n\n    #\n    # get the cumulated heat flow a at a certain age\n    #\n\n    def get_cumulated_heat_at_hours(self, target_h=4, cutoff_min=None):\n        \"\"\"\n        get the cumulated heat flow a at a certain age\n        \"\"\"\n\n        def applicable(df, target_h=4, cutoff_min=None):\n            # convert target time to seconds\n            target_s = 3600 * target_h\n            # helper\n            _helper = df.query(\"time_s &lt;= @target_s\").tail(1)\n            # get heat at target time\n            hf_at_target = float(_helper[\"normalized_heat_j_g\"].values[0])\n\n            # if cutoff time specified\n            if cutoff_min:\n                # convert target time to seconds\n                target_s = 60 * cutoff_min\n                try:\n                    # helper\n                    _helper = df.query(\"time_s &lt;= @target_s\").tail(1)\n                    # type conversion\n                    hf_at_cutoff = float(_helper[\"normalized_heat_j_g\"].values[0])\n                    # correct heatflow for heatflow at cutoff\n                    hf_at_target = hf_at_target - hf_at_cutoff\n                except TypeError:\n                    name_wt_nan = df[\"sample_short\"].tolist()[0]\n                    print(\n                        f\"Found NaN in Normalized heat of sample {name_wt_nan} searching for cumulated heat at {target_h}h and a cutoff of {cutoff_min}min.\"\n                    )\n                    return np.NaN\n\n            # return\n            return hf_at_target\n\n        # in case of one specified time\n        if isinstance(target_h, int) or isinstance(target_h, float):\n            # groupby\n            results = (\n                self._data.groupby(by=\"sample\")\n                .apply(\n                    lambda x: applicable(x, target_h=target_h, cutoff_min=cutoff_min),\n                    include_groups=False,\n                )\n                .reset_index(level=0)\n            )\n            # rename\n            results.columns = [\"sample\", \"cumulated_heat_at_hours\"]\n            results[\"target_h\"] = target_h\n            results[\"cutoff_min\"] = cutoff_min\n\n        # in case of specified list of times\n        elif isinstance(target_h, list):\n            # init list\n            list_of_results = []\n            # loop\n            for this_target_h in target_h:\n                # groupby\n                _results = (\n                    self._data.groupby(by=\"sample\")\n                    .apply(\n                        lambda x: applicable(\n                            x, target_h=this_target_h, cutoff_min=cutoff_min\n                        ),\n                        include_groups=False,\n                    )\n                    .reset_index(level=0)\n                )\n                # rename\n                _results.columns = [\"sample\", \"cumulated_heat_at_hours\"]\n                _results[\"target_h\"] = this_target_h\n                _results[\"cutoff_min\"] = cutoff_min\n                # append to list\n                list_of_results.append(_results)\n            # build overall results DataFrame\n            results = pd.concat(list_of_results)\n\n        # return\n        return results\n\n    #\n    # find peaks\n    #\n    def get_peaks(\n        self,\n        processparams,\n        target_col=\"normalized_heat_flow_w_g\",\n        regex=None,\n        cutoff_min=None,\n        show_plot=True,\n        plt_right_s=2e5,\n        plt_top=1e-2,\n        ax=None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        get DataFrame of peak characteristics.\n\n        Parameters\n        ----------\n        target_col : str, optional\n            measured quantity within which peaks are searched for. The default is \"normalized_heat_flow_w_g\"\n        regex : str, optional\n            regex pattern to include only certain experimental result files\n            during initialization. The default is None.\n        cutoff_min : int | float, optional\n            Time in minutes below which collected data points are discarded for peak picking\n        show_plot : bool, optional\n            Flag whether or not to plot peak picking for each sample. The default is True.\n        plt_right_s : int | float, optional\n            Upper limit of x-axis of in seconds. The default is 2e5.\n        plt_top : int | float, optional\n            Upper limit of y-axis of. The default is 1e-2.\n        ax : matplotlib.axes._axes.Axes | None, optional\n            The default is None.\n\n        Returns\n        -------\n        pd.DataFrame holding peak characterisitcs for each sample.\n\n        \"\"\"\n\n        # list of peaks\n        list_of_peaks_dfs = []\n\n        # loop samples\n        for sample, data in self._iter_samples(regex=regex):\n            # cutoff\n            if processparams.cutoff.cutoff_min:\n                # discard points at early age\n                data = data.query(\"time_s &gt;= @processparams.cutoff.cutoff_min * 60\")\n\n            # reset index\n            data = data.reset_index(drop=True)\n\n            # target_columns\n            _age_col = \"time_s\"\n            _target_col = target_col\n\n            # find peaks\n            peaks, properties = signal.find_peaks(\n                data[_target_col],\n                prominence=processparams.peakdetection.prominence,\n                distance=processparams.peakdetection.distance,\n            )\n\n            # plot?\n            if show_plot:\n                self._plot_peak_positions(\n                    data, ax, _age_col, _target_col, peaks, sample, plt_top, plt_right_s\n                )\n\n            # compile peak characteristics\n            peak_characteristics = pd.concat(\n                [\n                    data.iloc[peaks, :],\n                    pd.DataFrame(\n                        properties[\"prominences\"], index=peaks, columns=[\"prominence\"]\n                    ),\n                    pd.DataFrame({\"peak_nr\": np.arange((len(peaks)))}, index=peaks),\n                ],\n                axis=1,\n            )\n\n            # append\n            list_of_peaks_dfs.append(peak_characteristics)\n\n        # compile peak information\n        peaks = pd.concat(list_of_peaks_dfs)\n\n        if isinstance(ax, matplotlib.axes._axes.Axes):\n            # return peak list and ax\n            return peaks, ax\n        else:  # return peak list only\n            return peaks\n\n    #\n    # get peak onsets\n    #\n    def get_peak_onsets(\n        self,\n        target_col=\"normalized_heat_flow_w_g\",\n        age_col=\"time_s\",\n        time_discarded_s=900,\n        rolling=1,\n        gradient_threshold=0.0005,\n        show_plot=False,\n        exclude_discarded_time=False,\n        regex=None,\n        ax: plt.Axes = None,\n    ):\n        \"\"\"\n        get peak onsets based on a criterion of minimum gradient\n\n        Parameters\n        ----------\n        target_col : str, optional\n            measured quantity within which peak onsets are searched for. The default is \"normalized_heat_flow_w_g\"\n        age_col : str, optional\n            Time unit within which peak onsets are searched for. The default is \"time_s\"\n        time_discarded_s : int | float, optional\n            Time in seconds below which collected data points are discarded for peak onset picking. The default is 900.\n        rolling : int, optional\n            Width of \"rolling\" window within which the values of \"target_col\" are averaged. A higher value will introduce a stronger smoothing effect. The default is 1, i.e. no smoothing.\n        gradient_threshold : float, optional\n            Threshold of slope for identification of a peak onset. For a lower value, earlier peak onsets will be identified. The default is 0.0005.\n        show_plot : bool, optional\n            Flag whether or not to plot peak picking for each sample. The default is False.\n        exclude_discarded_time : bool, optional\n            Whether or not to discard the experimental values obtained before \"time_discarded_s\" also in the visualization. The default is False.\n        regex : str, optional\n            regex pattern to include only certain experimental result files during initialization. The default is None.\n        ax : matplotlib.axes._axes.Axes | None, optional\n            The default is None.\n        Returns\n        -------\n        pd.DataFrame holding peak onset characterisitcs for each sample.\n\n        \"\"\"\n\n        # init list of characteristics\n        list_of_characteristics = []\n\n        # loop samples\n        for sample, data in self._iter_samples(regex=regex):\n            if exclude_discarded_time:\n                # exclude\n                data = data.query(f\"{age_col} &gt;= {time_discarded_s}\")\n\n            # reset index\n            data = data.reset_index(drop=True)\n\n            # calculate get gradient\n            data[\"gradient\"] = pd.Series(\n                np.gradient(data[target_col].rolling(rolling).mean(), data[age_col])\n            )\n\n            # get relevant points\n            characteristics = data.copy()\n            # discard initial time\n            characteristics = characteristics.query(f\"{age_col} &gt;= {time_discarded_s}\")\n            # look at values with certain gradient only\n            characteristics = characteristics.query(\"gradient &gt; @gradient_threshold\")\n            # consider first entry exclusively\n            characteristics = characteristics.head(1)\n\n            # optional plotting\n            if show_plot:\n                # if specific axis to plot to is specified\n                if isinstance(ax, matplotlib.axes._axes.Axes):\n                    # plot heat flow curve\n                    p = ax.plot(data[age_col], data[target_col])\n\n                    # add vertical lines\n                    for _idx, _row in characteristics.iterrows():\n                        # vline\n                        ax.axvline(_row.at[age_col], color=p[0].get_color(), alpha=0.3)\n                        # add \"slope line\"\n                        ax.axline(\n                            (_row.at[age_col], _row.at[target_col]),\n                            slope=_row.at[\"gradient\"],\n                            color=p[0].get_color(),\n                            # color=\"k\",\n                            # linewidth=0.2\n                            alpha=0.25,\n                            linestyle=\"--\",\n                        )\n\n                    # cosmetics\n                    # ax.set_xscale(\"log\")\n                    ax.set_title(\"Onset for \" + pathlib.Path(sample).stem)\n                    ax.set_xlabel(age_col)\n                    ax.set_ylabel(target_col)\n\n                    ax.fill_between(\n                        [ax.get_ylim()[0], time_discarded_s],\n                        [ax.get_ylim()[0]] * 2,\n                        [ax.get_ylim()[1]] * 2,\n                        color=\"black\",\n                        alpha=0.35,\n                    )\n\n                    # set axis limit\n                    ax.set_xlim(left=100)\n\n                else:\n                    # plot heat flow curve\n                    plt.plot(data[age_col], data[target_col])\n\n                    # add vertical lines\n                    for _idx, _row in characteristics.iterrows():\n                        # vline\n                        plt.axvline(_row.at[age_col], color=\"red\", alpha=0.3)\n\n                    # cosmetics\n                    # plt.xscale(\"log\")\n                    plt.title(\"Onset for \" + pathlib.Path(sample).stem)\n                    plt.xlabel(age_col)\n                    plt.ylabel(target_col)\n\n                    # get axis\n                    ax = plt.gca()\n\n                    plt.fill_between(\n                        [ax.get_ylim()[0], time_discarded_s],\n                        [ax.get_ylim()[0]] * 2,\n                        [ax.get_ylim()[1]] * 2,\n                        color=\"black\",\n                        alpha=0.35,\n                    )\n\n                    # set axis limit\n                    plt.xlim(left=100)\n\n            # append to list\n            list_of_characteristics.append(characteristics)\n\n        # build overall list\n        onset_characteristics = pd.concat(list_of_characteristics)\n\n        # return\n        if isinstance(ax, matplotlib.axes._axes.Axes):\n            # return onset characteristics and ax\n            return onset_characteristics, ax\n        else:\n            # return onset characteristics exclusively\n            return onset_characteristics\n\n    #\n    # get maximum slope\n    #\n\n    def get_maximum_slope(\n        self,\n        processparams,\n        target_col=\"normalized_heat_flow_w_g\",\n        age_col=\"time_s\",\n        time_discarded_s=900,\n        show_plot=False,\n        show_info=True,\n        exclude_discarded_time=False,\n        regex=None,\n        read_start_c3s=False,\n        ax=None,\n        save_path=None,\n    ):\n        \"\"\"\n        get maximum slope as a characteristic value\n\n        Parameters\n        ----------\n        target_col : str, optional\n            measured quantity within which peak onsets are searched for. The default is \"normalized_heat_flow_w_g\"\n        age_col : str, optional\n            Time unit within which peak onsets are searched for. The default is \"time_s\"\n        time_discarded_s : int | float, optional\n            Time in seconds below which collected data points are discarded for peak onset picking. The default is 900.\n        show_plot : bool, optional\n            Flag whether or not to plot peak picking for each sample. The default is False.\n        exclude_discarded_time : bool, optional\n            Whether or not to discard the experimental values obtained before \"time_discarded_s\" also in the visualization. The default is False.\n        regex : str, optional\n            regex pattern to include only certain experimental result files during initialization. The default is None.\n        Returns\n        -------\n        pd.DataFrame holding peak onset characterisitcs for each sample.\n\n        \"\"\"\n\n        # init list of characteristics\n        list_of_characteristics = []\n\n        # loop samples\n        for sample, data in self._iter_samples(regex=regex):\n            sample_name = pathlib.Path(sample).stem\n            if exclude_discarded_time:\n                # exclude\n                data = data.query(f\"{age_col} &gt;= {time_discarded_s}\")\n\n            # manual definition of start time to look for c3s - in case auto peak detection becomes difficult\n            if read_start_c3s:\n                c3s_start_time_s = self._metadata.query(\n                    f\"sample_number == '{sample_name}'\"\n                )[\"t_c3s_min_s\"].values[0]\n                c3s_end_time_s = self._metadata.query(\n                    f\"sample_number == '{sample_name}'\"\n                )[\"t_c3s_max_s\"].values[0]\n                data = data.query(\n                    f\"{age_col} &gt;= {c3s_start_time_s} &amp; {age_col} &lt;= {c3s_end_time_s}\"\n                )\n\n            if show_info:\n                print(f\"Determineing maximum slope of {pathlib.Path(sample).stem}\")\n\n            processor = HeatFlowProcessor(processparams)\n\n            data = make_equidistant(data)\n\n            if processparams.rolling_mean.apply:\n                data = processor.apply_rolling_mean(data)\n\n            data[\"gradient\"], data[\"curvature\"] = (\n                processor.calculate_heatflow_derivatives(data)\n            )\n\n            characteristics = processor.get_largest_slope(data, processparams)\n            if characteristics.empty:\n                continue\n\n            # optional plotting\n            if show_plot:\n                self._plot_maximum_slope(\n                    data,\n                    ax,\n                    age_col,\n                    target_col,\n                    sample,\n                    characteristics,\n                    time_discarded_s,\n                    save_path=save_path,\n                )\n                # plot heat flow curve\n                # plt.plot(data[age_col], data[target_col], label=target_col)\n                # plt.plot(\n                #     data[age_col],\n                #     data[\"gradient\"] * 1e4 + 0.001,\n                #     label=\"gradient * 1e4 + 1mW\",\n                # )\n\n                # # add vertical lines\n                # for _idx, _row in characteristics.iterrows():\n                #     # vline\n                #     plt.axvline(_row.at[age_col], color=\"green\", alpha=0.3)\n\n                # # cosmetics\n                # plt.xscale(\"log\")\n                # plt.title(f\"Maximum slope plot for {pathlib.Path(sample).stem}\")\n                # plt.xlabel(age_col)\n                # plt.ylabel(target_col)\n                # plt.legend()\n\n                # # get axis\n                # ax = plt.gca()\n\n                # plt.fill_between(\n                #     [ax.get_ylim()[0], time_discarded_s],\n                #     [ax.get_ylim()[0]] * 2,\n                #     [ax.get_ylim()[1]] * 2,\n                #     color=\"black\",\n                #     alpha=0.35,\n                # )\n\n                # # set axis limit\n                # plt.xlim(left=100)\n                # plt.ylim(bottom=0, top=0.01)\n\n                # # show\n                # plt.show()\n\n            # append to list\n            list_of_characteristics.append(characteristics)\n\n        if not list_of_characteristics:\n            print(\"No maximum slope found, check you processing parameters\")\n        # build overall list\n        else:\n            max_slope_characteristics = pd.concat(list_of_characteristics)\n            # return\n            return max_slope_characteristics\n\n    #\n    # get reaction onset via maximum slope\n    #\n    def get_peak_onset_via_max_slope(\n        self,\n        processparams,\n        show_plot=False,\n        ax=None,\n    ):\n        \"\"\"\n        get reaction onset based on tangent of maximum heat flow and heat flow\n        during the dormant period. The characteristic time is inferred from\n        the intersection of both characteristic lines\n\n        Parameters\n        ----------\n        show_plot : TYPE, optional\n            DESCRIPTION. The default is False.\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n        # get onsets\n        max_slopes = self.get_maximum_slope(\n            processparams,\n        )\n        # % get dormant period HFs\n        dorm_hfs = self.get_dormant_period_heatflow(\n            processparams,  # cutoff_min=cutoff_min, prominence=prominence\n        )\n\n        # init list\n        list_onsets = []\n\n        # loop samples\n        for i, row in max_slopes.iterrows():\n            # calculate y-offset\n            t = row[\"normalized_heat_flow_w_g\"] - row[\"time_s\"] * row[\"gradient\"]\n            # calculate point of intersection\n            x_intersect = (\n                float(\n                    dorm_hfs[dorm_hfs[\"sample_short\"] == row[\"sample_short\"]][\n                        \"normalized_heat_flow_w_g\"\n                    ]\n                )\n                - t\n            ) / row[\"gradient\"]\n            # get maximum time value\n            tmax = self._data.query(\"sample_short == @row['sample_short']\")[\n                \"time_s\"\n            ].max()\n            # get maximum heat flow value\n            hmax = self._data.query(\n                \"time_s &gt; 3000 &amp; sample_short == @row['sample_short']\"\n            )[\"normalized_heat_flow_w_g\"].max()\n\n            # append to list\n            list_onsets.append(\n                {\n                    \"sample\": row[\"sample_short\"],\n                    \"onset_time_s\": x_intersect,\n                    \"onset_time_min\": x_intersect / 60,\n                }\n            )\n\n            if show_plot:\n                if isinstance(ax, matplotlib.axes._axes.Axes):\n                    # plot data\n                    ax = self.plot(\n                        t_unit=\"s\", y_unit_milli=False, regex=row[\"sample_short\"], ax=ax\n                    )\n                    ax.axline(\n                        (row[\"time_s\"], row[\"normalized_heat_flow_w_g\"]),\n                        slope=row[\"gradient\"],\n                        color=\"k\",\n                    )\n                    ax.axhline(\n                        float(\n                            dorm_hfs[dorm_hfs[\"sample_short\"] == row[\"sample_short\"]][\n                                \"normalized_heat_flow_w_g\"\n                            ]\n                        ),\n                        color=\"k\",\n                    )\n                    # guide to the eye line\n                    ax.axvline(x_intersect, color=\"red\")\n                    # info text\n                    ax.text(x_intersect, 0, f\" {x_intersect/60:.1f} min\\n\", color=\"red\")\n                    # ax limits\n                    ax.set_xlim(0, tmax)\n                    ax.set_ylim(0, hmax)\n                    # title\n                    ax.set_title(row[\"sample_short\"])\n\n                else:\n                    # plot data\n                    self.plot(\n                        t_unit=\"s\",\n                        y_unit_milli=False,\n                        regex=row[\"sample_short\"],\n                    )\n                    # max slope line\n                    plt.axline(\n                        (row[\"time_s\"], row[\"normalized_heat_flow_w_g\"]),\n                        slope=row[\"gradient\"],\n                        color=\"k\",\n                    )\n                    # dormant heat plot\n                    plt.axhline(\n                        float(\n                            dorm_hfs[dorm_hfs[\"sample_short\"] == row[\"sample_short\"]][\n                                \"normalized_heat_flow_w_g\"\n                            ]\n                        ),\n                        color=\"k\",\n                    )\n                    # guide to the eye line\n                    plt.axhline(0, alpha=0.5, linewidth=0.5, linestyle=\":\")\n                    # guide to the eye line\n                    plt.axvline(x_intersect, color=\"red\")\n                    # info text\n                    plt.text(\n                        x_intersect, 0, f\" {x_intersect/60:.1f} min\\n\", color=\"red\"\n                    )\n                    # ax limits\n                    plt.xlim(0, tmax)\n                    plt.ylim(0, hmax)\n                    # title\n                    plt.title(row[\"sample_short\"])\n                    plt.show()\n\n        # build overall dataframe to be returned\n        onsets = pd.DataFrame(list_onsets)\n\n        # merge with dorm_hfs\n        onsets = onsets.merge(\n            dorm_hfs[\n                [\"sample_short\", \"normalized_heat_flow_w_g\", \"normalized_heat_j_g\"]\n            ],\n            left_on=\"sample\",\n            right_on=\"sample_short\",\n            how=\"left\",\n        )\n\n        # rename\n        onsets = onsets.rename(\n            columns={\n                \"normalized_heat_flow_w_g\": \"normalized_heat_flow_w_g_at_dorm_min\",\n                \"normalized_heat_j_g\": \"normalized_heat_j_g_at_dorm_min\",\n            }\n        )\n\n        # return\n        if isinstance(ax, matplotlib.axes._axes.Axes):\n            # return onset characteristics and ax\n            return onsets, ax\n        else:\n            # return onset characteristics exclusively\n            return onsets\n\n    #\n    # get dormant period heatflow\n    #\n\n    def get_dormant_period_heatflow(\n        self,\n        processparams,\n        regex: str = None,\n        cutoff_min: int = 5,\n        upper_dormant_thresh_w_g: float = 0.002,\n        plot_right_boundary=2e5,\n        prominence: float = 1e-3,\n        show_plot=False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        get dormant period heatflow\n\n        Parameters\n        ----------\n        regex : str, optional\n            DESCRIPTION. The default is None.\n        cutoff_min : int, optional\n            DESCRIPTION. The default is 5.\n        upper_dormant_thresh_w_g : float, optional\n            DESCRIPTION. The default is 0.001.\n        show_plot : TYPE, optional\n            DESCRIPTION. The default is False.\n\n        Returns\n        -------\n        result : TYPE\n            DESCRIPTION.\n\n        \"\"\"\n\n        # init results list\n        list_dfs = []\n\n        # loop samples\n        for sample, data in self._iter_samples(regex=regex):\n            # get peak as \"right border\"\n            _peaks = self.get_peaks(\n                processparams,\n                # cutoff_min=cutoff_min,\n                regex=pathlib.Path(sample).name,\n                # prominence=processparams.gradient_peak_prominence, # prominence,\n                show_plot=True,\n            )\n\n            # identify \"dormant period\" as range between initial spike\n            # and first reaction peak\n\n            if show_plot:\n                # plot\n                plt.plot(\n                    data[\"time_s\"],\n                    data[\"normalized_heat_flow_w_g\"],\n                    # linestyle=\"\",\n                    # marker=\"o\",\n                )\n\n            # discard points at early age\n            data = data.query(\"time_s &gt;= @processparams.cutoff.cutoff_min * 60\")\n            if not _peaks.empty:\n                # discard points after the first peak\n                data = data.query('time_s &lt;= @_peaks[\"time_s\"].min()')\n\n            # reset index\n            data = data.reset_index(drop=True)\n\n            # pick relevant points at minimum heat flow\n            data = data.iloc[data[\"normalized_heat_flow_w_g\"].idxmin(), :].to_frame().T\n\n            if show_plot:\n                # guide to the eye lines\n                plt.axhline(float(data[\"normalized_heat_flow_w_g\"]), color=\"red\")\n                plt.axvline(float(data[\"time_s\"]), color=\"red\")\n                # indicate cutoff time\n                plt.axvspan(0, cutoff_min * 60, color=\"black\", alpha=0.5)\n                # limits\n                # plt.xlim(0, _peaks[\"time_s\"].min())\n                plt.xlim(0, plot_right_boundary)\n                plt.ylim(0, upper_dormant_thresh_w_g)\n                # title\n                plt.title(pathlib.Path(sample).stem)\n                # show\n                plt.show()\n\n            # add to list\n            list_dfs.append(data)\n\n        # convert to overall datafram\n        result = pd.concat(list_dfs).reset_index(drop=True)\n\n        # return\n        return result\n\n    #\n    # get ASTM C1679 characteristics\n    #\n\n    def get_astm_c1679_characteristics(\n        self,\n        processparams,\n        individual: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        get characteristics according to ASTM C1679. Compiles a list of data\n        points at half-maximum \"normalized heat flow\", wherein the half maximum\n        is either determined for each individual heat flow curve individually\n        or as the mean value if the heat flow curves considered.\n\n        Parameters\n        ----------\n        individual : bool, optional\n            DESCRIPTION. The default is False.\n\n        Returns\n        -------\n        astm_times : pd.DataFrame\n            DESCRIPTION.\n\n        \"\"\"\n\n        # get peaks\n        peaks = self.get_peaks(processparams, plt_right_s=4e5)\n        # sort peaks by ascending normalized heat flow\n        peaks = peaks.sort_values(by=\"normalized_heat_flow_w_g\", ascending=True)\n        # select highest peak --&gt; ASTM C1679\n        peaks = peaks.groupby(by=\"sample\").last()\n\n        # get data\n        data = self.get_data()\n\n        # init empty list for collecting characteristics\n        astm_times = []\n\n        # loop samples\n        for sample, sample_data in self._iter_samples():\n            # pick sample data\n            helper = data[data[\"sample\"] == sample]\n\n            # check if peak was found\n            if peaks[peaks[\"sample_short\"] == sample_data.sample_short[0]].empty:\n                helper = helper.iloc[0:1]\n                # manually set time to NaN to indicate that no peak was found\n                helper[\"time_s\"] = np.NaN\n\n            else:\n                # restrict to times before the peak\n                helper = helper[helper[\"time_s\"] &lt;= peaks.at[sample, \"time_s\"]]\n\n                # restrict to relevant heatflows the peak\n                if individual == True:\n                    helper = helper[\n                        helper[\"normalized_heat_flow_w_g\"]\n                        &lt;= peaks.at[sample, \"normalized_heat_flow_w_g\"] * 0.50\n                    ]\n                else:\n                    # use half-maximum average\n                    helper = helper[\n                        helper[\"normalized_heat_flow_w_g\"]\n                        &lt;= peaks[\"normalized_heat_flow_w_g\"].mean() * 0.50\n                    ]\n\n                # add to list of of selected points\n            astm_times.append(helper.tail(1))\n\n        # build overall DataFrame\n        astm_times = pd.concat(astm_times)\n\n        # return\n        return astm_times\n\n    #\n    # get data\n    #\n\n    def get_data(self):\n        \"\"\"\n        get data\n\n        Returns\n        -------\n        pd.DataFrame\n            data, i.e. heat flow, heat, sample, ....\n\n        \"\"\"\n\n        return self._data\n\n    #\n    # get information\n    #\n\n    def get_information(self):\n        \"\"\"\n        get information\n\n        Returns\n        -------\n        pd.DataFrame\n            information, i.e. date of measurement, operator, comment ...\n\n        \"\"\"\n\n        return self._info\n\n    #\n    # get added metadata\n    #\n    def get_metadata(self) -&gt; tuple:\n        \"\"\"\n\n\n         Returns\n         -------\n        tuple\n             pd.DataFrame of metadata and string of the column used as ID (has to\n             be unique).\n        \"\"\"\n\n        # return\n        return self._metadata, self._metadata_id\n\n    #\n    # get sample names\n    #\n\n    def get_sample_names(self):\n        \"\"\"\n        get list of sample names\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # get list\n        samples = [pathlib.Path(s).stem for s, _ in self._iter_samples()]\n\n        # return\n        return samples\n\n    #\n    # set\n    #\n\n    def normalize_sample_to_mass(\n        self, sample_short: str, mass_g: float, show_info=True\n    ):\n        \"\"\"\n        normalize \"heat_flow\" to a certain mass\n\n        Parameters\n        ----------\n        sample_short : str\n            \"sample_short\" name of sample to be normalized.\n        mass_g : float\n            mass in gram to which \"heat_flow_w\" are normalized.\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # normalize \"heat_flow_w\" to sample mass\n        self._data.loc[\n            self._data[\"sample_short\"] == sample_short, \"normalized_heat_flow_w_g\"\n        ] = (\n            self._data.loc[self._data[\"sample_short\"] == sample_short, \"heat_flow_w\"]\n            / mass_g\n        )\n\n        # normalize \"heat_j\" to sample mass\n        try:\n            self._data.loc[\n                self._data[\"sample_short\"] == sample_short, \"normalized_heat_j_g\"\n            ] = (\n                self._data.loc[self._data[\"sample_short\"] == sample_short, \"heat_j\"]\n                / mass_g\n            )\n        except Exception:\n            pass\n\n        # info\n        if show_info:\n            print(f\"Sample {sample_short} normalized to {mass_g}g sample.\")\n\n    #\n    # infer \"heat_j\" values\n    #\n\n    def _infer_heat_j_column(self):\n        \"\"\"\n        helper function to calculate the \"heat_j\" columns from \"heat_flow_w\" and\n        \"time_s\" columns\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # list of dfs\n        list_of_dfs = []\n\n        # loop samples\n        for sample, roi in self._iter_samples():\n            # check whether a \"native\" \"heat_j\"-column is available\n            try:\n                if not roi[\"heat_j\"].isna().all():\n                    # use as is\n                    list_of_dfs.append(roi)\n                    # go to next\n                    continue\n            except KeyError as e:\n                # info\n                print(e)\n\n            # info\n            print(f'==&gt; Inferring \"heat_j\" column for {sample}')\n\n            # get target rows\n            roi = roi.dropna(subset=[\"heat_flow_w\"]).sort_values(by=\"time_s\")\n\n            # inferring cumulated heat using the \"trapezoidal integration method\"\n\n            # introduce helpers\n            roi[\"_h1_y\"] = 0.5 * (\n                roi[\"heat_flow_w\"] + roi[\"heat_flow_w\"].shift(1)\n            ).shift(-1)\n            roi[\"_h2_x\"] = (roi[\"time_s\"] - roi[\"time_s\"].shift(1)).shift(-1)\n\n            # integrate\n            roi[\"heat_j\"] = (roi[\"_h1_y\"] * roi[\"_h2_x\"]).cumsum()\n\n            # clean\n            del roi[\"_h1_y\"], roi[\"_h2_x\"]\n\n            # append to list\n            list_of_dfs.append(roi)\n\n        # set data including \"heat_j\"\n        self._data = pd.concat(list_of_dfs)\n\n    #\n    # remove pickle files\n    #\n    def remove_pickle_files(self):\n        \"\"\"\n        remove pickle files if re-reading of source files needed\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # remove files\n        for file in [self._file_data_pickle, self._file_info_pickle]:\n            # remove file\n            pathlib.Path(file).unlink()\n\n    #\n    # add metadata\n    #\n    def add_metadata_source(self, file: str, sample_id_column: str):\n        \"\"\"\n        add an additional source of metadata the object. The source file is of\n        type \"csv\" or \"xlsx\" and holds information on one sample per row. Columns\n        can be named without restrictions.\n\n        To allow for a mapping, the values occurring in self._data[\"sample_short\"]\n        should appear in the source file. The column is declared via the keyword\n        \"sample_id_colum\"\n\n        Parameters\n        ----------\n        file : str\n            path to additonal metadata source file.\n        sample_id_column : str\n            column name in the additional source file matching self._data[\"sample_short\"].\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        if not pathlib.Path(file).suffix.lower() in [\".csv\", \".xlsx\"]:\n            # info\n            print(\"Please use metadata files of type csv and xlsx only.\")\n            # return\n            return\n\n        # read file\n        try:\n            # read as Excel\n            self._metadata = pd.read_excel(file)\n        except ValueError:\n            # read as csv\n            self._metadata = pd.read_csv(file)\n\n        # save mapper column\n        if sample_id_column in self._metadata.columns:\n            # save mapper column\n            self._metadata_id = sample_id_column\n        else:\n            # raise custom Exception\n            raise AddMetaDataSourceException(self._metadata.columns.tolist())\n\n    #\n    # get metadata group-by options\n    #\n    def get_metadata_grouping_options(self) -&gt; list:\n        \"\"\"\n        get a list of categories to group by in in \"self.plot_by_category\"\n\n        Returns\n        -------\n        list\n            list of categories avaialble for grouping by.\n        \"\"\"\n\n        # get list based on column names of \"_metadata\"\n        return self._metadata.columns.tolist()\n\n    #\n    # average by metadata\n    #\n    def average_by_metadata(\n        self,\n        group_by: str,\n        meta_id=\"experiment_nr\",\n        data_id=\"sample_short\",\n        time_average_window_s: int = None,\n        time_average_log_bin_count: int = None,\n        time_s_max: int = 2 * 24 * 60 * 60,\n        get_time_from=\"left\",\n        resampling_s: str = \"5s\",\n    ):\n        \"\"\"\n\n\n        Parameters\n        ----------\n        group_by : str | list[str]\n            DESCRIPTION.\n        meta_id : TYPE, optional\n            DESCRIPTION. The default is \"experiment_nr\".\n        data_id : TYPE, optional\n            DESCRIPTION. The default is \"sample_short\".\n        time_average_window_s : TYPE, optional\n            DESCRIPTION. The default is 60. The value is not(!) consindered if\n            the keyword time_average_log_bin_count is specified\n        get_time_from : TYPE, optional\n            DESCRIPTION. The default is \"left\". further options: # \"mid\" \"right\"\n\n        time_average_log_bin_count: number of bins if even spacing in logarithmic scale is applied\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # get metadata\n        meta, meta_id = self.get_metadata()\n\n        # get data\n        df = self._data\n\n        # make data equidistant grouped by sample_short\n        df = (\n            df.groupby(data_id)\n            .apply(lambda x: apply_resampling(x, resampling_s))\n            .reset_index(drop=True)\n        )\n\n        # rename sample in \"data\" by metadata grouping options\n        for value, group in meta.groupby(group_by):\n            # if one grouping level is used\n            if isinstance(value, str) or isinstance(value, int):\n                # modify data --&gt; replace \"sample_short\" with metadata group name\n                _idx_to_replace = df[data_id].isin(group[meta_id])\n                df.loc[_idx_to_replace, data_id] = str(value)\n            # if multiple grouping levels are used\n            elif isinstance(value, tuple):\n                # modify data --&gt; replace \"sample_short\" with metadata group name\n                _idx_to_replace = df[data_id].isin(group[meta_id])\n                df.loc[_idx_to_replace, data_id] = \" | \".join([str(x) for x in value])\n            else:\n                pass\n\n        # sort experimentally detected times to \"bins\"\n        if time_average_log_bin_count:\n            # evenly spaced bins on log scale (geometric spacing)\n            df[\"BIN\"] = pd.cut(\n                df[\"time_s\"],\n                np.geomspace(1, time_s_max, num=time_average_log_bin_count),\n            )\n        elif time_average_window_s:\n            # evenly spaced bins on linear scale with fixed width\n            df[\"BIN\"] = pd.cut(\n                df[\"time_s\"], np.arange(0, time_s_max, time_average_window_s)\n            )\n\n        if \"BIN\" in df.columns:\n            # calculate average and std\n            df = (\n                df.groupby([data_id, \"BIN\"])\n                .agg(\n                    {\n                        \"normalized_heat_flow_w_g\": [\"mean\", \"std\"],\n                        \"normalized_heat_j_g\": [\"mean\", \"std\"],\n                    }\n                )\n                .dropna(thresh=2)\n                .reset_index()\n            )\n        else:\n            # calculate average and std\n            df = (\n                df.groupby([data_id, \"time_s\"])\n                .agg(\n                    {\n                        \"normalized_heat_flow_w_g\": [\"mean\", \"std\"],\n                        \"normalized_heat_j_g\": [\"mean\", \"std\"],\n                    }\n                )\n                .dropna(thresh=2)\n                .reset_index()\n            )\n\n        # \"flatten\" column names\n        df.columns = [\"_\".join(i).replace(\"mean\", \"_\").strip(\"_\") for i in df.columns]\n\n        if \"BIN\" in df.columns:\n            # regain \"time_s\" columns\n            if get_time_from == \"left\":\n                df[\"time_s\"] = [i.left for i in df[\"BIN\"]]\n            elif get_time_from == \"mid\":\n                df[\"time_s\"] = [i.mid for i in df[\"BIN\"]]\n            elif get_time_from == \"right\":\n                df[\"time_s\"] = [i.right for i in df[\"BIN\"]]\n\n            # remove \"BIN\" auxiliary column\n            del df[\"BIN\"]\n\n        # copy information to \"sample\" column --&gt; needed for plotting\n        df[\"sample\"] = df[data_id]\n\n        # overwrite data with averaged data\n        self._data = df\n\n    #\n    # undo action of \"average_by_metadata\"\n    #\n    def undo_average_by_metadata(self):\n        \"\"\"\n        undo action of \"average_by_metadata\"\n        \"\"\"\n\n        # set \"unprocessed\" data as exeperimental data / \"de-average\"\n        if not self._data_unprocessed.empty:\n            # reset\n            self._data = self._data_unprocessed.copy()\n\n    #\n    # apply_tian_correction\n    #\n    def apply_tian_correction(\n        self,\n        processparams,  # tau=300, window=11, polynom=3, spline_smoothing_1st: float = 1e-9, spline_smoothing_2nd: float = 1e-9\n    ) -&gt; None:\n        \"\"\"\n        apply_tian_correction\n\n        Parameters\n        ----------\n\n        processparams :\n            ProcessingParameters object containing all processing parameters for calorimetry data.\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # apply the correction for each sample\n        for s, d in self._iter_samples():\n            # get y-data\n            y = d[\"normalized_heat_flow_w_g\"]\n            # NaN-handling in y-data\n            y = y.fillna(0)\n            # get x-data\n            x = d[\"time_s\"]\n\n            processor = HeatFlowProcessor(processparams)\n\n            dydx, dy2dx2 = processor.calculate_heatflow_derivatives(d)\n\n            if processparams.time_constants.tau2 == None:\n                # calculate corrected heatflow\n                norm_hf = (\n                    dydx * processparams.time_constants.tau1\n                    + self._data.loc[\n                        self._data[\"sample\"] == s, \"normalized_heat_flow_w_g\"\n                    ]\n                )\n            else:\n                # calculate corrected heatflow\n                norm_hf = (\n                    dydx\n                    * (\n                        processparams.time_constants.tau1\n                        + processparams.time_constants.tau2\n                    )\n                    + dy2dx2\n                    * processparams.time_constants.tau1\n                    * processparams.time_constants.tau2\n                    + d[\"normalized_heat_flow_w_g\"]\n                )\n\n            self._data.loc[\n                self._data[\"sample\"] == s, \"normalized_heat_flow_w_g_tian\"\n            ] = norm_hf\n\n            self._data.loc[\n                self._data[\"sample\"] == s, \"gradient_normalized_heat_flow_w_g\"\n            ] = dydx\n\n            # calculate corresponding cumulative heat\n            self._data.loc[self._data[\"sample\"] == s, \"normalized_heat_j_g_tian\"] = (\n                integrate.cumulative_trapezoid(norm_hf.fillna(0), x=x, initial=0)\n            )\n\n    #\n    # undo Tian-correction\n    #\n    def undo_tian_correction(self):\n        \"\"\"\n        undo_tian_correction; i.e. restore original data\n\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # call original restore function\n        self.undo_average_by_metadata()\n\n    def _apply_adaptive_downsampling(self):\n        \"\"\"\n        apply adaptive downsampling to data\n        \"\"\"\n\n        # define temporary empty DataFrame\n        df = pd.DataFrame()\n\n        # apply the correction for each sample\n        for s, d in self._iter_samples():\n            # print(d.sample_short[0])\n            # print(len(d))\n            d = d.dropna(subset=[\"normalized_heat_flow_w_g\"])\n            # apply adaptive downsampling\n            if not self.processparams.downsample.section_split:\n                d = adaptive_downsample(\n                    d,\n                    x_col=\"time_s\",\n                    y_col=\"normalized_heat_flow_w_g\",\n                    processparams=self.processparams,\n                )\n            else:\n                d = downsample_sections(\n                    d,\n                    x_col=\"time_s\",\n                    y_col=\"normalized_heat_flow_w_g\",\n                    processparams=self.processparams,\n                )\n            df = pd.concat([df, d])\n\n        # set data to downsampled data\n        self._data = df\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.__init__","title":"<code>__init__(folder=None, show_info=True, regex=None, auto_clean=False, cold_start=True, processparams=None)</code>","text":"<p>intialize measurements from folder</p> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def __init__(\n    self,\n    folder=None,\n    show_info=True,\n    regex=None,\n    auto_clean=False,\n    cold_start=True,\n    processparams=None,\n):\n    \"\"\"\n    intialize measurements from folder\n\n\n    \"\"\"\n\n    if not isinstance(processparams, ProcessingParameters):\n        self.processparams = ProcessingParameters()\n    else:\n        self.processparams = processparams\n\n    # read\n    if folder:\n        if cold_start:\n            # get data and parameters\n            self._get_data_and_parameters_from_folder(\n                folder, regex=regex, show_info=show_info\n            )\n        else:\n            # get data and parameters from pickled files\n            self._get_data_and_parameters_from_pickle()\n        try:\n            if auto_clean:\n                # remove NaN values and merge time columns\n                self._auto_clean_data()\n        except Exception as e:\n            # info\n            print(e)\n            raise AutoCleanException\n            # return\n            return\n\n    if self.processparams.downsample.apply:\n        self._apply_adaptive_downsampling()\n    # Message\n    print(\n        \"================\\nAre you missing some samples? Try rerunning with auto_clean=True and cold_start=True.\\n=================\"\n    )\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.add_metadata_source","title":"<code>add_metadata_source(file, sample_id_column)</code>","text":"<p>add an additional source of metadata the object. The source file is of type \"csv\" or \"xlsx\" and holds information on one sample per row. Columns can be named without restrictions.</p> <p>To allow for a mapping, the values occurring in self._data[\"sample_short\"] should appear in the source file. The column is declared via the keyword \"sample_id_colum\"</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>path to additonal metadata source file.</p> required <code>sample_id_column</code> <code>str</code> <p>column name in the additional source file matching self._data[\"sample_short\"].</p> required <p>Returns:</p> Type Description <code>None.</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def add_metadata_source(self, file: str, sample_id_column: str):\n    \"\"\"\n    add an additional source of metadata the object. The source file is of\n    type \"csv\" or \"xlsx\" and holds information on one sample per row. Columns\n    can be named without restrictions.\n\n    To allow for a mapping, the values occurring in self._data[\"sample_short\"]\n    should appear in the source file. The column is declared via the keyword\n    \"sample_id_colum\"\n\n    Parameters\n    ----------\n    file : str\n        path to additonal metadata source file.\n    sample_id_column : str\n        column name in the additional source file matching self._data[\"sample_short\"].\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    if not pathlib.Path(file).suffix.lower() in [\".csv\", \".xlsx\"]:\n        # info\n        print(\"Please use metadata files of type csv and xlsx only.\")\n        # return\n        return\n\n    # read file\n    try:\n        # read as Excel\n        self._metadata = pd.read_excel(file)\n    except ValueError:\n        # read as csv\n        self._metadata = pd.read_csv(file)\n\n    # save mapper column\n    if sample_id_column in self._metadata.columns:\n        # save mapper column\n        self._metadata_id = sample_id_column\n    else:\n        # raise custom Exception\n        raise AddMetaDataSourceException(self._metadata.columns.tolist())\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.apply_tian_correction","title":"<code>apply_tian_correction(processparams)</code>","text":"<p>apply_tian_correction</p> <p>Parameters:</p> Name Type Description Default <code>processparams</code> <p>ProcessingParameters object containing all processing parameters for calorimetry data.</p> required <p>Returns:</p> Type Description <code>None.</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def apply_tian_correction(\n    self,\n    processparams,  # tau=300, window=11, polynom=3, spline_smoothing_1st: float = 1e-9, spline_smoothing_2nd: float = 1e-9\n) -&gt; None:\n    \"\"\"\n    apply_tian_correction\n\n    Parameters\n    ----------\n\n    processparams :\n        ProcessingParameters object containing all processing parameters for calorimetry data.\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    # apply the correction for each sample\n    for s, d in self._iter_samples():\n        # get y-data\n        y = d[\"normalized_heat_flow_w_g\"]\n        # NaN-handling in y-data\n        y = y.fillna(0)\n        # get x-data\n        x = d[\"time_s\"]\n\n        processor = HeatFlowProcessor(processparams)\n\n        dydx, dy2dx2 = processor.calculate_heatflow_derivatives(d)\n\n        if processparams.time_constants.tau2 == None:\n            # calculate corrected heatflow\n            norm_hf = (\n                dydx * processparams.time_constants.tau1\n                + self._data.loc[\n                    self._data[\"sample\"] == s, \"normalized_heat_flow_w_g\"\n                ]\n            )\n        else:\n            # calculate corrected heatflow\n            norm_hf = (\n                dydx\n                * (\n                    processparams.time_constants.tau1\n                    + processparams.time_constants.tau2\n                )\n                + dy2dx2\n                * processparams.time_constants.tau1\n                * processparams.time_constants.tau2\n                + d[\"normalized_heat_flow_w_g\"]\n            )\n\n        self._data.loc[\n            self._data[\"sample\"] == s, \"normalized_heat_flow_w_g_tian\"\n        ] = norm_hf\n\n        self._data.loc[\n            self._data[\"sample\"] == s, \"gradient_normalized_heat_flow_w_g\"\n        ] = dydx\n\n        # calculate corresponding cumulative heat\n        self._data.loc[self._data[\"sample\"] == s, \"normalized_heat_j_g_tian\"] = (\n            integrate.cumulative_trapezoid(norm_hf.fillna(0), x=x, initial=0)\n        )\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.average_by_metadata","title":"<code>average_by_metadata(group_by, meta_id='experiment_nr', data_id='sample_short', time_average_window_s=None, time_average_log_bin_count=None, time_s_max=2 * 24 * 60 * 60, get_time_from='left', resampling_s='5s')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>group_by</code> <code>str | list[str]</code> <p>DESCRIPTION.</p> required <code>meta_id</code> <code>TYPE</code> <p>DESCRIPTION. The default is \"experiment_nr\".</p> <code>'experiment_nr'</code> <code>data_id</code> <code>TYPE</code> <p>DESCRIPTION. The default is \"sample_short\".</p> <code>'sample_short'</code> <code>time_average_window_s</code> <code>TYPE</code> <p>DESCRIPTION. The default is 60. The value is not(!) consindered if the keyword time_average_log_bin_count is specified</p> <code>None</code> <code>get_time_from</code> <code>TYPE</code> <p>DESCRIPTION. The default is \"left\". further options: # \"mid\" \"right\"</p> <code>'left'</code> <code>time_average_log_bin_count</code> <code>int</code> <code>None</code> <p>Returns:</p> Type Description <code>None.</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def average_by_metadata(\n    self,\n    group_by: str,\n    meta_id=\"experiment_nr\",\n    data_id=\"sample_short\",\n    time_average_window_s: int = None,\n    time_average_log_bin_count: int = None,\n    time_s_max: int = 2 * 24 * 60 * 60,\n    get_time_from=\"left\",\n    resampling_s: str = \"5s\",\n):\n    \"\"\"\n\n\n    Parameters\n    ----------\n    group_by : str | list[str]\n        DESCRIPTION.\n    meta_id : TYPE, optional\n        DESCRIPTION. The default is \"experiment_nr\".\n    data_id : TYPE, optional\n        DESCRIPTION. The default is \"sample_short\".\n    time_average_window_s : TYPE, optional\n        DESCRIPTION. The default is 60. The value is not(!) consindered if\n        the keyword time_average_log_bin_count is specified\n    get_time_from : TYPE, optional\n        DESCRIPTION. The default is \"left\". further options: # \"mid\" \"right\"\n\n    time_average_log_bin_count: number of bins if even spacing in logarithmic scale is applied\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    # get metadata\n    meta, meta_id = self.get_metadata()\n\n    # get data\n    df = self._data\n\n    # make data equidistant grouped by sample_short\n    df = (\n        df.groupby(data_id)\n        .apply(lambda x: apply_resampling(x, resampling_s))\n        .reset_index(drop=True)\n    )\n\n    # rename sample in \"data\" by metadata grouping options\n    for value, group in meta.groupby(group_by):\n        # if one grouping level is used\n        if isinstance(value, str) or isinstance(value, int):\n            # modify data --&gt; replace \"sample_short\" with metadata group name\n            _idx_to_replace = df[data_id].isin(group[meta_id])\n            df.loc[_idx_to_replace, data_id] = str(value)\n        # if multiple grouping levels are used\n        elif isinstance(value, tuple):\n            # modify data --&gt; replace \"sample_short\" with metadata group name\n            _idx_to_replace = df[data_id].isin(group[meta_id])\n            df.loc[_idx_to_replace, data_id] = \" | \".join([str(x) for x in value])\n        else:\n            pass\n\n    # sort experimentally detected times to \"bins\"\n    if time_average_log_bin_count:\n        # evenly spaced bins on log scale (geometric spacing)\n        df[\"BIN\"] = pd.cut(\n            df[\"time_s\"],\n            np.geomspace(1, time_s_max, num=time_average_log_bin_count),\n        )\n    elif time_average_window_s:\n        # evenly spaced bins on linear scale with fixed width\n        df[\"BIN\"] = pd.cut(\n            df[\"time_s\"], np.arange(0, time_s_max, time_average_window_s)\n        )\n\n    if \"BIN\" in df.columns:\n        # calculate average and std\n        df = (\n            df.groupby([data_id, \"BIN\"])\n            .agg(\n                {\n                    \"normalized_heat_flow_w_g\": [\"mean\", \"std\"],\n                    \"normalized_heat_j_g\": [\"mean\", \"std\"],\n                }\n            )\n            .dropna(thresh=2)\n            .reset_index()\n        )\n    else:\n        # calculate average and std\n        df = (\n            df.groupby([data_id, \"time_s\"])\n            .agg(\n                {\n                    \"normalized_heat_flow_w_g\": [\"mean\", \"std\"],\n                    \"normalized_heat_j_g\": [\"mean\", \"std\"],\n                }\n            )\n            .dropna(thresh=2)\n            .reset_index()\n        )\n\n    # \"flatten\" column names\n    df.columns = [\"_\".join(i).replace(\"mean\", \"_\").strip(\"_\") for i in df.columns]\n\n    if \"BIN\" in df.columns:\n        # regain \"time_s\" columns\n        if get_time_from == \"left\":\n            df[\"time_s\"] = [i.left for i in df[\"BIN\"]]\n        elif get_time_from == \"mid\":\n            df[\"time_s\"] = [i.mid for i in df[\"BIN\"]]\n        elif get_time_from == \"right\":\n            df[\"time_s\"] = [i.right for i in df[\"BIN\"]]\n\n        # remove \"BIN\" auxiliary column\n        del df[\"BIN\"]\n\n    # copy information to \"sample\" column --&gt; needed for plotting\n    df[\"sample\"] = df[data_id]\n\n    # overwrite data with averaged data\n    self._data = df\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.get_astm_c1679_characteristics","title":"<code>get_astm_c1679_characteristics(processparams, individual=False)</code>","text":"<p>get characteristics according to ASTM C1679. Compiles a list of data points at half-maximum \"normalized heat flow\", wherein the half maximum is either determined for each individual heat flow curve individually or as the mean value if the heat flow curves considered.</p> <p>Parameters:</p> Name Type Description Default <code>individual</code> <code>bool</code> <p>DESCRIPTION. The default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>astm_times</code> <code>DataFrame</code> <p>DESCRIPTION.</p> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def get_astm_c1679_characteristics(\n    self,\n    processparams,\n    individual: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    get characteristics according to ASTM C1679. Compiles a list of data\n    points at half-maximum \"normalized heat flow\", wherein the half maximum\n    is either determined for each individual heat flow curve individually\n    or as the mean value if the heat flow curves considered.\n\n    Parameters\n    ----------\n    individual : bool, optional\n        DESCRIPTION. The default is False.\n\n    Returns\n    -------\n    astm_times : pd.DataFrame\n        DESCRIPTION.\n\n    \"\"\"\n\n    # get peaks\n    peaks = self.get_peaks(processparams, plt_right_s=4e5)\n    # sort peaks by ascending normalized heat flow\n    peaks = peaks.sort_values(by=\"normalized_heat_flow_w_g\", ascending=True)\n    # select highest peak --&gt; ASTM C1679\n    peaks = peaks.groupby(by=\"sample\").last()\n\n    # get data\n    data = self.get_data()\n\n    # init empty list for collecting characteristics\n    astm_times = []\n\n    # loop samples\n    for sample, sample_data in self._iter_samples():\n        # pick sample data\n        helper = data[data[\"sample\"] == sample]\n\n        # check if peak was found\n        if peaks[peaks[\"sample_short\"] == sample_data.sample_short[0]].empty:\n            helper = helper.iloc[0:1]\n            # manually set time to NaN to indicate that no peak was found\n            helper[\"time_s\"] = np.NaN\n\n        else:\n            # restrict to times before the peak\n            helper = helper[helper[\"time_s\"] &lt;= peaks.at[sample, \"time_s\"]]\n\n            # restrict to relevant heatflows the peak\n            if individual == True:\n                helper = helper[\n                    helper[\"normalized_heat_flow_w_g\"]\n                    &lt;= peaks.at[sample, \"normalized_heat_flow_w_g\"] * 0.50\n                ]\n            else:\n                # use half-maximum average\n                helper = helper[\n                    helper[\"normalized_heat_flow_w_g\"]\n                    &lt;= peaks[\"normalized_heat_flow_w_g\"].mean() * 0.50\n                ]\n\n            # add to list of of selected points\n        astm_times.append(helper.tail(1))\n\n    # build overall DataFrame\n    astm_times = pd.concat(astm_times)\n\n    # return\n    return astm_times\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.get_cumulated_heat_at_hours","title":"<code>get_cumulated_heat_at_hours(target_h=4, cutoff_min=None)</code>","text":"<p>get the cumulated heat flow a at a certain age</p> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def get_cumulated_heat_at_hours(self, target_h=4, cutoff_min=None):\n    \"\"\"\n    get the cumulated heat flow a at a certain age\n    \"\"\"\n\n    def applicable(df, target_h=4, cutoff_min=None):\n        # convert target time to seconds\n        target_s = 3600 * target_h\n        # helper\n        _helper = df.query(\"time_s &lt;= @target_s\").tail(1)\n        # get heat at target time\n        hf_at_target = float(_helper[\"normalized_heat_j_g\"].values[0])\n\n        # if cutoff time specified\n        if cutoff_min:\n            # convert target time to seconds\n            target_s = 60 * cutoff_min\n            try:\n                # helper\n                _helper = df.query(\"time_s &lt;= @target_s\").tail(1)\n                # type conversion\n                hf_at_cutoff = float(_helper[\"normalized_heat_j_g\"].values[0])\n                # correct heatflow for heatflow at cutoff\n                hf_at_target = hf_at_target - hf_at_cutoff\n            except TypeError:\n                name_wt_nan = df[\"sample_short\"].tolist()[0]\n                print(\n                    f\"Found NaN in Normalized heat of sample {name_wt_nan} searching for cumulated heat at {target_h}h and a cutoff of {cutoff_min}min.\"\n                )\n                return np.NaN\n\n        # return\n        return hf_at_target\n\n    # in case of one specified time\n    if isinstance(target_h, int) or isinstance(target_h, float):\n        # groupby\n        results = (\n            self._data.groupby(by=\"sample\")\n            .apply(\n                lambda x: applicable(x, target_h=target_h, cutoff_min=cutoff_min),\n                include_groups=False,\n            )\n            .reset_index(level=0)\n        )\n        # rename\n        results.columns = [\"sample\", \"cumulated_heat_at_hours\"]\n        results[\"target_h\"] = target_h\n        results[\"cutoff_min\"] = cutoff_min\n\n    # in case of specified list of times\n    elif isinstance(target_h, list):\n        # init list\n        list_of_results = []\n        # loop\n        for this_target_h in target_h:\n            # groupby\n            _results = (\n                self._data.groupby(by=\"sample\")\n                .apply(\n                    lambda x: applicable(\n                        x, target_h=this_target_h, cutoff_min=cutoff_min\n                    ),\n                    include_groups=False,\n                )\n                .reset_index(level=0)\n            )\n            # rename\n            _results.columns = [\"sample\", \"cumulated_heat_at_hours\"]\n            _results[\"target_h\"] = this_target_h\n            _results[\"cutoff_min\"] = cutoff_min\n            # append to list\n            list_of_results.append(_results)\n        # build overall results DataFrame\n        results = pd.concat(list_of_results)\n\n    # return\n    return results\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.get_data","title":"<code>get_data()</code>","text":"<p>get data</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>data, i.e. heat flow, heat, sample, ....</p> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def get_data(self):\n    \"\"\"\n    get data\n\n    Returns\n    -------\n    pd.DataFrame\n        data, i.e. heat flow, heat, sample, ....\n\n    \"\"\"\n\n    return self._data\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.get_dormant_period_heatflow","title":"<code>get_dormant_period_heatflow(processparams, regex=None, cutoff_min=5, upper_dormant_thresh_w_g=0.002, plot_right_boundary=200000.0, prominence=0.001, show_plot=False)</code>","text":"<p>get dormant period heatflow</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>DESCRIPTION. The default is None.</p> <code>None</code> <code>cutoff_min</code> <code>int</code> <p>DESCRIPTION. The default is 5.</p> <code>5</code> <code>upper_dormant_thresh_w_g</code> <code>float</code> <p>DESCRIPTION. The default is 0.001.</p> <code>0.002</code> <code>show_plot</code> <code>TYPE</code> <p>DESCRIPTION. The default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>result</code> <code>TYPE</code> <p>DESCRIPTION.</p> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def get_dormant_period_heatflow(\n    self,\n    processparams,\n    regex: str = None,\n    cutoff_min: int = 5,\n    upper_dormant_thresh_w_g: float = 0.002,\n    plot_right_boundary=2e5,\n    prominence: float = 1e-3,\n    show_plot=False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    get dormant period heatflow\n\n    Parameters\n    ----------\n    regex : str, optional\n        DESCRIPTION. The default is None.\n    cutoff_min : int, optional\n        DESCRIPTION. The default is 5.\n    upper_dormant_thresh_w_g : float, optional\n        DESCRIPTION. The default is 0.001.\n    show_plot : TYPE, optional\n        DESCRIPTION. The default is False.\n\n    Returns\n    -------\n    result : TYPE\n        DESCRIPTION.\n\n    \"\"\"\n\n    # init results list\n    list_dfs = []\n\n    # loop samples\n    for sample, data in self._iter_samples(regex=regex):\n        # get peak as \"right border\"\n        _peaks = self.get_peaks(\n            processparams,\n            # cutoff_min=cutoff_min,\n            regex=pathlib.Path(sample).name,\n            # prominence=processparams.gradient_peak_prominence, # prominence,\n            show_plot=True,\n        )\n\n        # identify \"dormant period\" as range between initial spike\n        # and first reaction peak\n\n        if show_plot:\n            # plot\n            plt.plot(\n                data[\"time_s\"],\n                data[\"normalized_heat_flow_w_g\"],\n                # linestyle=\"\",\n                # marker=\"o\",\n            )\n\n        # discard points at early age\n        data = data.query(\"time_s &gt;= @processparams.cutoff.cutoff_min * 60\")\n        if not _peaks.empty:\n            # discard points after the first peak\n            data = data.query('time_s &lt;= @_peaks[\"time_s\"].min()')\n\n        # reset index\n        data = data.reset_index(drop=True)\n\n        # pick relevant points at minimum heat flow\n        data = data.iloc[data[\"normalized_heat_flow_w_g\"].idxmin(), :].to_frame().T\n\n        if show_plot:\n            # guide to the eye lines\n            plt.axhline(float(data[\"normalized_heat_flow_w_g\"]), color=\"red\")\n            plt.axvline(float(data[\"time_s\"]), color=\"red\")\n            # indicate cutoff time\n            plt.axvspan(0, cutoff_min * 60, color=\"black\", alpha=0.5)\n            # limits\n            # plt.xlim(0, _peaks[\"time_s\"].min())\n            plt.xlim(0, plot_right_boundary)\n            plt.ylim(0, upper_dormant_thresh_w_g)\n            # title\n            plt.title(pathlib.Path(sample).stem)\n            # show\n            plt.show()\n\n        # add to list\n        list_dfs.append(data)\n\n    # convert to overall datafram\n    result = pd.concat(list_dfs).reset_index(drop=True)\n\n    # return\n    return result\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.get_information","title":"<code>get_information()</code>","text":"<p>get information</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>information, i.e. date of measurement, operator, comment ...</p> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def get_information(self):\n    \"\"\"\n    get information\n\n    Returns\n    -------\n    pd.DataFrame\n        information, i.e. date of measurement, operator, comment ...\n\n    \"\"\"\n\n    return self._info\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.get_maximum_slope","title":"<code>get_maximum_slope(processparams, target_col='normalized_heat_flow_w_g', age_col='time_s', time_discarded_s=900, show_plot=False, show_info=True, exclude_discarded_time=False, regex=None, read_start_c3s=False, ax=None, save_path=None)</code>","text":"<p>get maximum slope as a characteristic value</p> <p>Parameters:</p> Name Type Description Default <code>target_col</code> <code>str</code> <p>measured quantity within which peak onsets are searched for. The default is \"normalized_heat_flow_w_g\"</p> <code>'normalized_heat_flow_w_g'</code> <code>age_col</code> <code>str</code> <p>Time unit within which peak onsets are searched for. The default is \"time_s\"</p> <code>'time_s'</code> <code>time_discarded_s</code> <code>int | float</code> <p>Time in seconds below which collected data points are discarded for peak onset picking. The default is 900.</p> <code>900</code> <code>show_plot</code> <code>bool</code> <p>Flag whether or not to plot peak picking for each sample. The default is False.</p> <code>False</code> <code>exclude_discarded_time</code> <code>bool</code> <p>Whether or not to discard the experimental values obtained before \"time_discarded_s\" also in the visualization. The default is False.</p> <code>False</code> <code>regex</code> <code>str</code> <p>regex pattern to include only certain experimental result files during initialization. The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame holding peak onset characterisitcs for each sample.</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def get_maximum_slope(\n    self,\n    processparams,\n    target_col=\"normalized_heat_flow_w_g\",\n    age_col=\"time_s\",\n    time_discarded_s=900,\n    show_plot=False,\n    show_info=True,\n    exclude_discarded_time=False,\n    regex=None,\n    read_start_c3s=False,\n    ax=None,\n    save_path=None,\n):\n    \"\"\"\n    get maximum slope as a characteristic value\n\n    Parameters\n    ----------\n    target_col : str, optional\n        measured quantity within which peak onsets are searched for. The default is \"normalized_heat_flow_w_g\"\n    age_col : str, optional\n        Time unit within which peak onsets are searched for. The default is \"time_s\"\n    time_discarded_s : int | float, optional\n        Time in seconds below which collected data points are discarded for peak onset picking. The default is 900.\n    show_plot : bool, optional\n        Flag whether or not to plot peak picking for each sample. The default is False.\n    exclude_discarded_time : bool, optional\n        Whether or not to discard the experimental values obtained before \"time_discarded_s\" also in the visualization. The default is False.\n    regex : str, optional\n        regex pattern to include only certain experimental result files during initialization. The default is None.\n    Returns\n    -------\n    pd.DataFrame holding peak onset characterisitcs for each sample.\n\n    \"\"\"\n\n    # init list of characteristics\n    list_of_characteristics = []\n\n    # loop samples\n    for sample, data in self._iter_samples(regex=regex):\n        sample_name = pathlib.Path(sample).stem\n        if exclude_discarded_time:\n            # exclude\n            data = data.query(f\"{age_col} &gt;= {time_discarded_s}\")\n\n        # manual definition of start time to look for c3s - in case auto peak detection becomes difficult\n        if read_start_c3s:\n            c3s_start_time_s = self._metadata.query(\n                f\"sample_number == '{sample_name}'\"\n            )[\"t_c3s_min_s\"].values[0]\n            c3s_end_time_s = self._metadata.query(\n                f\"sample_number == '{sample_name}'\"\n            )[\"t_c3s_max_s\"].values[0]\n            data = data.query(\n                f\"{age_col} &gt;= {c3s_start_time_s} &amp; {age_col} &lt;= {c3s_end_time_s}\"\n            )\n\n        if show_info:\n            print(f\"Determineing maximum slope of {pathlib.Path(sample).stem}\")\n\n        processor = HeatFlowProcessor(processparams)\n\n        data = make_equidistant(data)\n\n        if processparams.rolling_mean.apply:\n            data = processor.apply_rolling_mean(data)\n\n        data[\"gradient\"], data[\"curvature\"] = (\n            processor.calculate_heatflow_derivatives(data)\n        )\n\n        characteristics = processor.get_largest_slope(data, processparams)\n        if characteristics.empty:\n            continue\n\n        # optional plotting\n        if show_plot:\n            self._plot_maximum_slope(\n                data,\n                ax,\n                age_col,\n                target_col,\n                sample,\n                characteristics,\n                time_discarded_s,\n                save_path=save_path,\n            )\n            # plot heat flow curve\n            # plt.plot(data[age_col], data[target_col], label=target_col)\n            # plt.plot(\n            #     data[age_col],\n            #     data[\"gradient\"] * 1e4 + 0.001,\n            #     label=\"gradient * 1e4 + 1mW\",\n            # )\n\n            # # add vertical lines\n            # for _idx, _row in characteristics.iterrows():\n            #     # vline\n            #     plt.axvline(_row.at[age_col], color=\"green\", alpha=0.3)\n\n            # # cosmetics\n            # plt.xscale(\"log\")\n            # plt.title(f\"Maximum slope plot for {pathlib.Path(sample).stem}\")\n            # plt.xlabel(age_col)\n            # plt.ylabel(target_col)\n            # plt.legend()\n\n            # # get axis\n            # ax = plt.gca()\n\n            # plt.fill_between(\n            #     [ax.get_ylim()[0], time_discarded_s],\n            #     [ax.get_ylim()[0]] * 2,\n            #     [ax.get_ylim()[1]] * 2,\n            #     color=\"black\",\n            #     alpha=0.35,\n            # )\n\n            # # set axis limit\n            # plt.xlim(left=100)\n            # plt.ylim(bottom=0, top=0.01)\n\n            # # show\n            # plt.show()\n\n        # append to list\n        list_of_characteristics.append(characteristics)\n\n    if not list_of_characteristics:\n        print(\"No maximum slope found, check you processing parameters\")\n    # build overall list\n    else:\n        max_slope_characteristics = pd.concat(list_of_characteristics)\n        # return\n        return max_slope_characteristics\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.get_metadata","title":"<code>get_metadata()</code>","text":"Returns <p>tuple      pd.DataFrame of metadata and string of the column used as ID (has to      be unique).</p> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def get_metadata(self) -&gt; tuple:\n    \"\"\"\n\n\n     Returns\n     -------\n    tuple\n         pd.DataFrame of metadata and string of the column used as ID (has to\n         be unique).\n    \"\"\"\n\n    # return\n    return self._metadata, self._metadata_id\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.get_metadata_grouping_options","title":"<code>get_metadata_grouping_options()</code>","text":"<p>get a list of categories to group by in in \"self.plot_by_category\"</p> <p>Returns:</p> Type Description <code>list</code> <p>list of categories avaialble for grouping by.</p> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def get_metadata_grouping_options(self) -&gt; list:\n    \"\"\"\n    get a list of categories to group by in in \"self.plot_by_category\"\n\n    Returns\n    -------\n    list\n        list of categories avaialble for grouping by.\n    \"\"\"\n\n    # get list based on column names of \"_metadata\"\n    return self._metadata.columns.tolist()\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.get_peak_onset_via_max_slope","title":"<code>get_peak_onset_via_max_slope(processparams, show_plot=False, ax=None)</code>","text":"<p>get reaction onset based on tangent of maximum heat flow and heat flow during the dormant period. The characteristic time is inferred from the intersection of both characteristic lines</p> <p>Parameters:</p> Name Type Description Default <code>show_plot</code> <code>TYPE</code> <p>DESCRIPTION. The default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None.</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def get_peak_onset_via_max_slope(\n    self,\n    processparams,\n    show_plot=False,\n    ax=None,\n):\n    \"\"\"\n    get reaction onset based on tangent of maximum heat flow and heat flow\n    during the dormant period. The characteristic time is inferred from\n    the intersection of both characteristic lines\n\n    Parameters\n    ----------\n    show_plot : TYPE, optional\n        DESCRIPTION. The default is False.\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n    # get onsets\n    max_slopes = self.get_maximum_slope(\n        processparams,\n    )\n    # % get dormant period HFs\n    dorm_hfs = self.get_dormant_period_heatflow(\n        processparams,  # cutoff_min=cutoff_min, prominence=prominence\n    )\n\n    # init list\n    list_onsets = []\n\n    # loop samples\n    for i, row in max_slopes.iterrows():\n        # calculate y-offset\n        t = row[\"normalized_heat_flow_w_g\"] - row[\"time_s\"] * row[\"gradient\"]\n        # calculate point of intersection\n        x_intersect = (\n            float(\n                dorm_hfs[dorm_hfs[\"sample_short\"] == row[\"sample_short\"]][\n                    \"normalized_heat_flow_w_g\"\n                ]\n            )\n            - t\n        ) / row[\"gradient\"]\n        # get maximum time value\n        tmax = self._data.query(\"sample_short == @row['sample_short']\")[\n            \"time_s\"\n        ].max()\n        # get maximum heat flow value\n        hmax = self._data.query(\n            \"time_s &gt; 3000 &amp; sample_short == @row['sample_short']\"\n        )[\"normalized_heat_flow_w_g\"].max()\n\n        # append to list\n        list_onsets.append(\n            {\n                \"sample\": row[\"sample_short\"],\n                \"onset_time_s\": x_intersect,\n                \"onset_time_min\": x_intersect / 60,\n            }\n        )\n\n        if show_plot:\n            if isinstance(ax, matplotlib.axes._axes.Axes):\n                # plot data\n                ax = self.plot(\n                    t_unit=\"s\", y_unit_milli=False, regex=row[\"sample_short\"], ax=ax\n                )\n                ax.axline(\n                    (row[\"time_s\"], row[\"normalized_heat_flow_w_g\"]),\n                    slope=row[\"gradient\"],\n                    color=\"k\",\n                )\n                ax.axhline(\n                    float(\n                        dorm_hfs[dorm_hfs[\"sample_short\"] == row[\"sample_short\"]][\n                            \"normalized_heat_flow_w_g\"\n                        ]\n                    ),\n                    color=\"k\",\n                )\n                # guide to the eye line\n                ax.axvline(x_intersect, color=\"red\")\n                # info text\n                ax.text(x_intersect, 0, f\" {x_intersect/60:.1f} min\\n\", color=\"red\")\n                # ax limits\n                ax.set_xlim(0, tmax)\n                ax.set_ylim(0, hmax)\n                # title\n                ax.set_title(row[\"sample_short\"])\n\n            else:\n                # plot data\n                self.plot(\n                    t_unit=\"s\",\n                    y_unit_milli=False,\n                    regex=row[\"sample_short\"],\n                )\n                # max slope line\n                plt.axline(\n                    (row[\"time_s\"], row[\"normalized_heat_flow_w_g\"]),\n                    slope=row[\"gradient\"],\n                    color=\"k\",\n                )\n                # dormant heat plot\n                plt.axhline(\n                    float(\n                        dorm_hfs[dorm_hfs[\"sample_short\"] == row[\"sample_short\"]][\n                            \"normalized_heat_flow_w_g\"\n                        ]\n                    ),\n                    color=\"k\",\n                )\n                # guide to the eye line\n                plt.axhline(0, alpha=0.5, linewidth=0.5, linestyle=\":\")\n                # guide to the eye line\n                plt.axvline(x_intersect, color=\"red\")\n                # info text\n                plt.text(\n                    x_intersect, 0, f\" {x_intersect/60:.1f} min\\n\", color=\"red\"\n                )\n                # ax limits\n                plt.xlim(0, tmax)\n                plt.ylim(0, hmax)\n                # title\n                plt.title(row[\"sample_short\"])\n                plt.show()\n\n    # build overall dataframe to be returned\n    onsets = pd.DataFrame(list_onsets)\n\n    # merge with dorm_hfs\n    onsets = onsets.merge(\n        dorm_hfs[\n            [\"sample_short\", \"normalized_heat_flow_w_g\", \"normalized_heat_j_g\"]\n        ],\n        left_on=\"sample\",\n        right_on=\"sample_short\",\n        how=\"left\",\n    )\n\n    # rename\n    onsets = onsets.rename(\n        columns={\n            \"normalized_heat_flow_w_g\": \"normalized_heat_flow_w_g_at_dorm_min\",\n            \"normalized_heat_j_g\": \"normalized_heat_j_g_at_dorm_min\",\n        }\n    )\n\n    # return\n    if isinstance(ax, matplotlib.axes._axes.Axes):\n        # return onset characteristics and ax\n        return onsets, ax\n    else:\n        # return onset characteristics exclusively\n        return onsets\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.get_peak_onsets","title":"<code>get_peak_onsets(target_col='normalized_heat_flow_w_g', age_col='time_s', time_discarded_s=900, rolling=1, gradient_threshold=0.0005, show_plot=False, exclude_discarded_time=False, regex=None, ax=None)</code>","text":"<p>get peak onsets based on a criterion of minimum gradient</p> <p>Parameters:</p> Name Type Description Default <code>target_col</code> <code>str</code> <p>measured quantity within which peak onsets are searched for. The default is \"normalized_heat_flow_w_g\"</p> <code>'normalized_heat_flow_w_g'</code> <code>age_col</code> <code>str</code> <p>Time unit within which peak onsets are searched for. The default is \"time_s\"</p> <code>'time_s'</code> <code>time_discarded_s</code> <code>int | float</code> <p>Time in seconds below which collected data points are discarded for peak onset picking. The default is 900.</p> <code>900</code> <code>rolling</code> <code>int</code> <p>Width of \"rolling\" window within which the values of \"target_col\" are averaged. A higher value will introduce a stronger smoothing effect. The default is 1, i.e. no smoothing.</p> <code>1</code> <code>gradient_threshold</code> <code>float</code> <p>Threshold of slope for identification of a peak onset. For a lower value, earlier peak onsets will be identified. The default is 0.0005.</p> <code>0.0005</code> <code>show_plot</code> <code>bool</code> <p>Flag whether or not to plot peak picking for each sample. The default is False.</p> <code>False</code> <code>exclude_discarded_time</code> <code>bool</code> <p>Whether or not to discard the experimental values obtained before \"time_discarded_s\" also in the visualization. The default is False.</p> <code>False</code> <code>regex</code> <code>str</code> <p>regex pattern to include only certain experimental result files during initialization. The default is None.</p> <code>None</code> <code>ax</code> <code>Axes | None</code> <p>The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame holding peak onset characterisitcs for each sample.</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def get_peak_onsets(\n    self,\n    target_col=\"normalized_heat_flow_w_g\",\n    age_col=\"time_s\",\n    time_discarded_s=900,\n    rolling=1,\n    gradient_threshold=0.0005,\n    show_plot=False,\n    exclude_discarded_time=False,\n    regex=None,\n    ax: plt.Axes = None,\n):\n    \"\"\"\n    get peak onsets based on a criterion of minimum gradient\n\n    Parameters\n    ----------\n    target_col : str, optional\n        measured quantity within which peak onsets are searched for. The default is \"normalized_heat_flow_w_g\"\n    age_col : str, optional\n        Time unit within which peak onsets are searched for. The default is \"time_s\"\n    time_discarded_s : int | float, optional\n        Time in seconds below which collected data points are discarded for peak onset picking. The default is 900.\n    rolling : int, optional\n        Width of \"rolling\" window within which the values of \"target_col\" are averaged. A higher value will introduce a stronger smoothing effect. The default is 1, i.e. no smoothing.\n    gradient_threshold : float, optional\n        Threshold of slope for identification of a peak onset. For a lower value, earlier peak onsets will be identified. The default is 0.0005.\n    show_plot : bool, optional\n        Flag whether or not to plot peak picking for each sample. The default is False.\n    exclude_discarded_time : bool, optional\n        Whether or not to discard the experimental values obtained before \"time_discarded_s\" also in the visualization. The default is False.\n    regex : str, optional\n        regex pattern to include only certain experimental result files during initialization. The default is None.\n    ax : matplotlib.axes._axes.Axes | None, optional\n        The default is None.\n    Returns\n    -------\n    pd.DataFrame holding peak onset characterisitcs for each sample.\n\n    \"\"\"\n\n    # init list of characteristics\n    list_of_characteristics = []\n\n    # loop samples\n    for sample, data in self._iter_samples(regex=regex):\n        if exclude_discarded_time:\n            # exclude\n            data = data.query(f\"{age_col} &gt;= {time_discarded_s}\")\n\n        # reset index\n        data = data.reset_index(drop=True)\n\n        # calculate get gradient\n        data[\"gradient\"] = pd.Series(\n            np.gradient(data[target_col].rolling(rolling).mean(), data[age_col])\n        )\n\n        # get relevant points\n        characteristics = data.copy()\n        # discard initial time\n        characteristics = characteristics.query(f\"{age_col} &gt;= {time_discarded_s}\")\n        # look at values with certain gradient only\n        characteristics = characteristics.query(\"gradient &gt; @gradient_threshold\")\n        # consider first entry exclusively\n        characteristics = characteristics.head(1)\n\n        # optional plotting\n        if show_plot:\n            # if specific axis to plot to is specified\n            if isinstance(ax, matplotlib.axes._axes.Axes):\n                # plot heat flow curve\n                p = ax.plot(data[age_col], data[target_col])\n\n                # add vertical lines\n                for _idx, _row in characteristics.iterrows():\n                    # vline\n                    ax.axvline(_row.at[age_col], color=p[0].get_color(), alpha=0.3)\n                    # add \"slope line\"\n                    ax.axline(\n                        (_row.at[age_col], _row.at[target_col]),\n                        slope=_row.at[\"gradient\"],\n                        color=p[0].get_color(),\n                        # color=\"k\",\n                        # linewidth=0.2\n                        alpha=0.25,\n                        linestyle=\"--\",\n                    )\n\n                # cosmetics\n                # ax.set_xscale(\"log\")\n                ax.set_title(\"Onset for \" + pathlib.Path(sample).stem)\n                ax.set_xlabel(age_col)\n                ax.set_ylabel(target_col)\n\n                ax.fill_between(\n                    [ax.get_ylim()[0], time_discarded_s],\n                    [ax.get_ylim()[0]] * 2,\n                    [ax.get_ylim()[1]] * 2,\n                    color=\"black\",\n                    alpha=0.35,\n                )\n\n                # set axis limit\n                ax.set_xlim(left=100)\n\n            else:\n                # plot heat flow curve\n                plt.plot(data[age_col], data[target_col])\n\n                # add vertical lines\n                for _idx, _row in characteristics.iterrows():\n                    # vline\n                    plt.axvline(_row.at[age_col], color=\"red\", alpha=0.3)\n\n                # cosmetics\n                # plt.xscale(\"log\")\n                plt.title(\"Onset for \" + pathlib.Path(sample).stem)\n                plt.xlabel(age_col)\n                plt.ylabel(target_col)\n\n                # get axis\n                ax = plt.gca()\n\n                plt.fill_between(\n                    [ax.get_ylim()[0], time_discarded_s],\n                    [ax.get_ylim()[0]] * 2,\n                    [ax.get_ylim()[1]] * 2,\n                    color=\"black\",\n                    alpha=0.35,\n                )\n\n                # set axis limit\n                plt.xlim(left=100)\n\n        # append to list\n        list_of_characteristics.append(characteristics)\n\n    # build overall list\n    onset_characteristics = pd.concat(list_of_characteristics)\n\n    # return\n    if isinstance(ax, matplotlib.axes._axes.Axes):\n        # return onset characteristics and ax\n        return onset_characteristics, ax\n    else:\n        # return onset characteristics exclusively\n        return onset_characteristics\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.get_peaks","title":"<code>get_peaks(processparams, target_col='normalized_heat_flow_w_g', regex=None, cutoff_min=None, show_plot=True, plt_right_s=200000.0, plt_top=0.01, ax=None)</code>","text":"<p>get DataFrame of peak characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>target_col</code> <code>str</code> <p>measured quantity within which peaks are searched for. The default is \"normalized_heat_flow_w_g\"</p> <code>'normalized_heat_flow_w_g'</code> <code>regex</code> <code>str</code> <p>regex pattern to include only certain experimental result files during initialization. The default is None.</p> <code>None</code> <code>cutoff_min</code> <code>int | float</code> <p>Time in minutes below which collected data points are discarded for peak picking</p> <code>None</code> <code>show_plot</code> <code>bool</code> <p>Flag whether or not to plot peak picking for each sample. The default is True.</p> <code>True</code> <code>plt_right_s</code> <code>int | float</code> <p>Upper limit of x-axis of in seconds. The default is 2e5.</p> <code>200000.0</code> <code>plt_top</code> <code>int | float</code> <p>Upper limit of y-axis of. The default is 1e-2.</p> <code>0.01</code> <code>ax</code> <code>Axes | None</code> <p>The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame holding peak characterisitcs for each sample.</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def get_peaks(\n    self,\n    processparams,\n    target_col=\"normalized_heat_flow_w_g\",\n    regex=None,\n    cutoff_min=None,\n    show_plot=True,\n    plt_right_s=2e5,\n    plt_top=1e-2,\n    ax=None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    get DataFrame of peak characteristics.\n\n    Parameters\n    ----------\n    target_col : str, optional\n        measured quantity within which peaks are searched for. The default is \"normalized_heat_flow_w_g\"\n    regex : str, optional\n        regex pattern to include only certain experimental result files\n        during initialization. The default is None.\n    cutoff_min : int | float, optional\n        Time in minutes below which collected data points are discarded for peak picking\n    show_plot : bool, optional\n        Flag whether or not to plot peak picking for each sample. The default is True.\n    plt_right_s : int | float, optional\n        Upper limit of x-axis of in seconds. The default is 2e5.\n    plt_top : int | float, optional\n        Upper limit of y-axis of. The default is 1e-2.\n    ax : matplotlib.axes._axes.Axes | None, optional\n        The default is None.\n\n    Returns\n    -------\n    pd.DataFrame holding peak characterisitcs for each sample.\n\n    \"\"\"\n\n    # list of peaks\n    list_of_peaks_dfs = []\n\n    # loop samples\n    for sample, data in self._iter_samples(regex=regex):\n        # cutoff\n        if processparams.cutoff.cutoff_min:\n            # discard points at early age\n            data = data.query(\"time_s &gt;= @processparams.cutoff.cutoff_min * 60\")\n\n        # reset index\n        data = data.reset_index(drop=True)\n\n        # target_columns\n        _age_col = \"time_s\"\n        _target_col = target_col\n\n        # find peaks\n        peaks, properties = signal.find_peaks(\n            data[_target_col],\n            prominence=processparams.peakdetection.prominence,\n            distance=processparams.peakdetection.distance,\n        )\n\n        # plot?\n        if show_plot:\n            self._plot_peak_positions(\n                data, ax, _age_col, _target_col, peaks, sample, plt_top, plt_right_s\n            )\n\n        # compile peak characteristics\n        peak_characteristics = pd.concat(\n            [\n                data.iloc[peaks, :],\n                pd.DataFrame(\n                    properties[\"prominences\"], index=peaks, columns=[\"prominence\"]\n                ),\n                pd.DataFrame({\"peak_nr\": np.arange((len(peaks)))}, index=peaks),\n            ],\n            axis=1,\n        )\n\n        # append\n        list_of_peaks_dfs.append(peak_characteristics)\n\n    # compile peak information\n    peaks = pd.concat(list_of_peaks_dfs)\n\n    if isinstance(ax, matplotlib.axes._axes.Axes):\n        # return peak list and ax\n        return peaks, ax\n    else:  # return peak list only\n        return peaks\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.get_sample_names","title":"<code>get_sample_names()</code>","text":"<p>get list of sample names</p> <p>Returns:</p> Type Description <code>None.</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def get_sample_names(self):\n    \"\"\"\n    get list of sample names\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    # get list\n    samples = [pathlib.Path(s).stem for s, _ in self._iter_samples()]\n\n    # return\n    return samples\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.normalize_sample_to_mass","title":"<code>normalize_sample_to_mass(sample_short, mass_g, show_info=True)</code>","text":"<p>normalize \"heat_flow\" to a certain mass</p> <p>Parameters:</p> Name Type Description Default <code>sample_short</code> <code>str</code> <p>\"sample_short\" name of sample to be normalized.</p> required <code>mass_g</code> <code>float</code> <p>mass in gram to which \"heat_flow_w\" are normalized.</p> required <p>Returns:</p> Type Description <code>None.</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def normalize_sample_to_mass(\n    self, sample_short: str, mass_g: float, show_info=True\n):\n    \"\"\"\n    normalize \"heat_flow\" to a certain mass\n\n    Parameters\n    ----------\n    sample_short : str\n        \"sample_short\" name of sample to be normalized.\n    mass_g : float\n        mass in gram to which \"heat_flow_w\" are normalized.\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    # normalize \"heat_flow_w\" to sample mass\n    self._data.loc[\n        self._data[\"sample_short\"] == sample_short, \"normalized_heat_flow_w_g\"\n    ] = (\n        self._data.loc[self._data[\"sample_short\"] == sample_short, \"heat_flow_w\"]\n        / mass_g\n    )\n\n    # normalize \"heat_j\" to sample mass\n    try:\n        self._data.loc[\n            self._data[\"sample_short\"] == sample_short, \"normalized_heat_j_g\"\n        ] = (\n            self._data.loc[self._data[\"sample_short\"] == sample_short, \"heat_j\"]\n            / mass_g\n        )\n    except Exception:\n        pass\n\n    # info\n    if show_info:\n        print(f\"Sample {sample_short} normalized to {mass_g}g sample.\")\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.plot","title":"<code>plot(t_unit='h', y='normalized_heat_flow_w_g', y_unit_milli=True, regex=None, show_info=True, ax=None)</code>","text":"<p>Plot the calorimetry data.</p> <p>Parameters:</p> Name Type Description Default <code>t_unit</code> <code>str</code> <p>time unit. The default is \"h\". Options are \"s\", \"min\", \"h\", \"d\".</p> <code>'h'</code> <code>y</code> <code>str</code> <p>y-axis. The default is \"normalized_heat_flow_w_g\". Options are \"normalized_heat_flow_w_g\", \"heat_flow_w\", \"normalized_heat_j_g\", \"heat_j\".</p> <code>'normalized_heat_flow_w_g'</code> <code>y_unit_milli</code> <code>bool</code> <p>whether or not to plot y-axis in Milliwatt. The default is True.</p> <code>True</code> <code>regex</code> <code>str</code> <p>regex pattern to include only certain samples during plotting. The default is None.</p> <code>None</code> <code>show_info</code> <code>bool</code> <p>whether or not to show information. The default is True.</p> <code>True</code> <code>ax</code> <code>Axes</code> <p>axis to plot to. The default is None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import CaloCem as ta\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt;\n&gt;&gt;&gt; calodatapath = Path(__file__).parent\n&gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n&gt;&gt;&gt; tam.plot(t_unit=\"h\", y=\"normalized_heat_flow_w_g\", y_unit_milli=False)\n</code></pre> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def plot(\n    self,\n    t_unit=\"h\",\n    y=\"normalized_heat_flow_w_g\",\n    y_unit_milli=True,\n    regex=None,\n    show_info=True,\n    ax=None,\n):\n    \"\"\"\n\n    Plot the calorimetry data.\n\n    Parameters\n    ----------\n    t_unit : str, optional\n        time unit. The default is \"h\". Options are \"s\", \"min\", \"h\", \"d\".\n    y : str, optional\n        y-axis. The default is \"normalized_heat_flow_w_g\". Options are\n        \"normalized_heat_flow_w_g\", \"heat_flow_w\", \"normalized_heat_j_g\",\n        \"heat_j\".\n    y_unit_milli : bool, optional\n        whether or not to plot y-axis in Milliwatt. The default is True.\n    regex : str, optional\n        regex pattern to include only certain samples during plotting. The\n        default is None.\n    show_info : bool, optional\n        whether or not to show information. The default is True.\n    ax : matplotlib.axes._axes.Axes, optional\n        axis to plot to. The default is None.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import CaloCem as ta\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; calodatapath = Path(__file__).parent\n    &gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n    &gt;&gt;&gt; tam.plot(t_unit=\"h\", y=\"normalized_heat_flow_w_g\", y_unit_milli=False)\n\n    \"\"\"\n\n    # y-value\n    if y == \"normalized_heat_flow_w_g\":\n        y_column = \"normalized_heat_flow_w_g\"\n        y_label = \"Normalized Heat Flow / [W/g]\"\n    elif y == \"heat_flow_w\":\n        y_column = \"heat_flow_w\"\n        y_label = \"Heat Flow / [W]\"\n    elif y == \"normalized_heat_j_g\":\n        y_column = \"normalized_heat_j_g\"\n        y_label = \"Normalized Heat / [J/g]\"\n    elif y == \"heat_j\":\n        y_column = \"heat_j\"\n        y_label = \"Heat / [J]\"\n\n    if y_unit_milli:\n        y_label = y_label.replace(\"[\", \"[m\")\n\n    # x-unit\n    if t_unit == \"s\":\n        x_factor = 1.0\n    elif t_unit == \"min\":\n        x_factor = 1 / 60\n    elif t_unit == \"h\":\n        x_factor = 1 / (60 * 60)\n    elif t_unit == \"d\":\n        x_factor = 1 / (60 * 60 * 24)\n\n    # y-unit\n    if y_unit_milli:\n        y_factor = 1000\n    else:\n        y_factor = 1\n\n    for sample, data in self._iter_samples():\n        if regex:\n            if not re.findall(rf\"{regex}\", os.path.basename(sample)):\n                continue\n        data[\"time_s\"] = data[\"time_s\"] * x_factor\n        # all columns containing heat\n        heatcols = [s for s in data.columns if \"heat\" in s]\n        data[heatcols] = data[heatcols] * y_factor\n        ax, _ = utils.create_base_plot(data, ax, \"time_s\", y_column, sample)\n        ax = utils.style_base_plot(\n            ax,\n            y_label,\n            t_unit,\n            sample,\n        )\n    return ax\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.plot_by_category","title":"<code>plot_by_category(categories, t_unit='h', y='normalized_heat_flow_w_g', y_unit_milli=True)</code>","text":"<p>plot by category, wherein the category is based on the information passed via \"self._add_metadata_source\". Options available as \"category\" are accessible via \"self.get_metadata_grouping_options\"</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>(str, list[str])</code> <p>category (from \"self.get_metadata_grouping_options\") to group by. specify a string or a list of strings here</p> required <code>t_unit</code> <code>TYPE</code> <p>see \"self.plot\". The default is \"h\".</p> <code>'h'</code> <code>y</code> <code>TYPE</code> <p>see \"self.plot\". The default is \"normalized_heat_flow_w_g\".</p> <code>'normalized_heat_flow_w_g'</code> <code>y_unit_milli</code> <code>TYPE</code> <p>see \"self.plot\". The default is True.</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import CaloCem as ta\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt;\n&gt;&gt;&gt; calodatapath = Path(__file__).parent\n&gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n&gt;&gt;&gt; tam.plot_by_category(categories=\"sample\")\n</code></pre> <p>Returns:</p> Type Description <code>None.</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def plot_by_category(\n    self, categories, t_unit=\"h\", y=\"normalized_heat_flow_w_g\", y_unit_milli=True\n):\n    \"\"\"\n    plot by category, wherein the category is based on the information passed\n    via \"self._add_metadata_source\". Options available as \"category\" are\n    accessible via \"self.get_metadata_grouping_options\"\n\n    Parameters\n    ----------\n    categories : str, list[str]\n        category (from \"self.get_metadata_grouping_options\") to group by.\n        specify a string or a list of strings here\n    t_unit : TYPE, optional\n        see \"self.plot\". The default is \"h\".\n    y : TYPE, optional\n        see \"self.plot\". The default is \"normalized_heat_flow_w_g\".\n    y_unit_milli : TYPE, optional\n        see \"self.plot\". The default is True.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import CaloCem as ta\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; calodatapath = Path(__file__).parent\n    &gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n    &gt;&gt;&gt; tam.plot_by_category(categories=\"sample\")\n\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    def build_helper_string(values: list) -&gt; str:\n        \"\"\"\n        build a \"nicely\" formatted string from a supplied list\n        \"\"\"\n\n        if len(values) == 2:\n            # connect with \"and\"\n            formatted = \" and \".join([str(i) for i in values])\n        elif len(values) &gt; 2:\n            # connect with comma and \"and\" for last element\n            formatted = (\n                \", \".join([str(i) for i in values[:-1]]) + \" and \" + str(values[-1])\n            )\n        else:\n            formatted = \"---\"\n\n        # return\n        return formatted\n\n    # loop category values\n    for selections, _ in self._metadata.groupby(by=categories):\n        if isinstance(selections, tuple):\n            # - if multiple categories to group by are specified -\n            # init helper DataFrame\n            target_idx = pd.DataFrame()\n            # identify corresponding samples\n            for selection, category in zip(selections, categories):\n                target_idx[category] = self._metadata[category] == selection\n            # get relevant indices\n            target_idx = target_idx.sum(axis=1) == len(categories)\n            # define title\n            title = f\"Grouped by { build_helper_string(categories)} ({build_helper_string(selections)})\"\n        else:\n            # - if only one(!) category to group by is specified -\n            # identify corresponding samples\n            target_idx = self._metadata[categories] == selections\n            # define title\n            title = f\"Grouped by {categories} ({selections})\"\n\n        # pick relevant samples\n        target_samples = self._metadata.loc[target_idx, self._metadata_id]\n\n        # build corresponding regex\n        regex = \"(\" + \")|(\".join(target_samples) + \")\"\n\n        # plot\n        ax = self.plot(regex=regex, t_unit=t_unit, y=y, y_unit_milli=y_unit_milli)\n\n        # set title\n        ax.set_title(title)\n\n        # yield latest plot\n        yield selections, ax\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.remove_pickle_files","title":"<code>remove_pickle_files()</code>","text":"<p>remove pickle files if re-reading of source files needed</p> <p>Returns:</p> Type Description <code>None.</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def remove_pickle_files(self):\n    \"\"\"\n    remove pickle files if re-reading of source files needed\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    # remove files\n    for file in [self._file_data_pickle, self._file_info_pickle]:\n        # remove file\n        pathlib.Path(file).unlink()\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.undo_average_by_metadata","title":"<code>undo_average_by_metadata()</code>","text":"<p>undo action of \"average_by_metadata\"</p> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def undo_average_by_metadata(self):\n    \"\"\"\n    undo action of \"average_by_metadata\"\n    \"\"\"\n\n    # set \"unprocessed\" data as exeperimental data / \"de-average\"\n    if not self._data_unprocessed.empty:\n        # reset\n        self._data = self._data_unprocessed.copy()\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.Measurement.undo_tian_correction","title":"<code>undo_tian_correction()</code>","text":"<p>undo_tian_correction; i.e. restore original data</p> <p>Returns:</p> Type Description <code>None.</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def undo_tian_correction(self):\n    \"\"\"\n    undo_tian_correction; i.e. restore original data\n\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    # call original restore function\n    self.undo_average_by_metadata()\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.ProcessingParameters","title":"<code>ProcessingParameters</code>  <code>dataclass</code>","text":"<p>A data class for storing all processing parameters for calorimetry data.</p> <p>This class aggregates various processing parameters, including cutoff criteria, time constants for the Tian correction, and parameters for peak detection and gradient peak detection.</p> <p>Attributes:</p> Name Type Description <code>cutoff</code> <code>CutOffParameters</code> <p>Parameters defining the cutoff criteria for the analysis. Currently only cutoff_min is implemented, which defines the minimum time in minutes for the analysis. The default value is defined in the CutOffParameters class.</p> <code>time_constants</code> <code>TianCorrectionParameters</code> <p>Parameters related to time constants used in Tian's correction method for thermal analysis. he default values are defined in the TianCorrectionParameters class.</p> <code>peakdetection</code> <code>PeakDetectionParameters</code> <p>Parameters for detecting peaks in the thermal analysis data. This includes settings such as the minimum prominence and distance between peaks. The default values are defined in the PeakDetectionParameters class.</p> <code>gradient_peakdetection</code> <code>GradientPeakDetectionParameters</code> <p>Parameters for detecting peaks based on the gradient of the thermal analysis data. This includes more nuanced settings such as prominence, distance, width, relative height, and the criteria for selecting peaks (e.g., first peak, largest width). The default values are defined in the GradientPeakDetectionParameters class.</p> <code>downsample</code> <code>DownSamplingParameters</code> <p>Parameters for adaptive downsampling of the thermal analysis data. This includes settings such as the number of points, smoothing factor, and baseline weight. The default values are defined in the DownSamplingParameters class.</p> <p>Examples:</p> <p>Define a set of processing parameters for thermal analysis data.</p> <pre><code>&gt;&gt;&gt; processparams = ProcessingParameters()\n&gt;&gt;&gt; processparams.cutoff.cutoff_min = 30\n</code></pre> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>@dataclass\nclass ProcessingParameters:\n    \"\"\"\n    A data class for storing all processing parameters for calorimetry data.\n\n    This class aggregates various processing parameters, including cutoff criteria, time constants for the Tian correction, and parameters for peak detection and gradient peak detection.\n\n    Attributes\n    ----------\n\n    cutoff :\n        Parameters defining the cutoff criteria for the analysis.\n        Currently only cutoff_min is implemented, which defines the minimum time in minutes for the analysis. The default value is defined in the CutOffParameters class.\n\n    time_constants : TianCorrectionParameters\n        Parameters related to time constants used in Tian's correction method for thermal analysis. he default values are defined in the\n        TianCorrectionParameters class.\n\n    peakdetection : PeakDetectionParameters\n        Parameters for detecting peaks in the thermal analysis data. This includes settings such as the minimum\n        prominence and distance between peaks. The default values are defined in the PeakDetectionParameters class.\n\n    gradient_peakdetection : GradientPeakDetectionParameters\n        Parameters for detecting peaks based on the gradient of the thermal analysis data. This includes more\n        nuanced settings such as prominence, distance, width, relative height, and the criteria for selecting peaks\n        (e.g., first peak, largest width). The default values are defined in the GradientPeakDetectionParameters class.\n\n    downsample : DownSamplingParameters\n        Parameters for adaptive downsampling of the thermal analysis data. This includes settings such as the number of points,\n        smoothing factor, and baseline weight. The default values are defined in the DownSamplingParameters class.\n\n\n\n    Examples\n    --------\n\n    Define a set of processing parameters for thermal analysis data.\n\n    &gt;&gt;&gt; processparams = ProcessingParameters()\n    &gt;&gt;&gt; processparams.cutoff.cutoff_min = 30\n    \"\"\"\n\n    cutoff: CutOffParameters = field(default_factory=CutOffParameters)\n    time_constants: TianCorrectionParameters = field(\n        default_factory=TianCorrectionParameters\n    )\n\n    # peak detection params\n    peakdetection: PeakDetectionParameters = field(\n        default_factory=PeakDetectionParameters\n    )\n    gradient_peakdetection: GradientPeakDetectionParameters = field(\n        default_factory=GradientPeakDetectionParameters\n    )\n\n    # smoothing params\n    rolling_mean: RollingMeanParameters = field(default_factory=RollingMeanParameters)\n    median_filter: MedianFilterParameters = field(\n        default_factory=MedianFilterParameters\n    )\n    nonlin_savgol: NonLinSavGolParameters = field(\n        default_factory=NonLinSavGolParameters\n    )\n    spline_interpolation: SplineInterpolationParameters = field(\n        default_factory=SplineInterpolationParameters\n    )\n    downsample: DownSamplingParameters = field(default_factory=DownSamplingParameters)\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.SplineInterpolationParameters","title":"<code>SplineInterpolationParameters</code>  <code>dataclass</code>","text":"<p>Parameters for spline interpolation of heat flow data.</p> <p>Parameters:</p> Name Type Description Default <code>apply</code> <code>bool</code> <p>Flag indicating whether spline interpolation should be applied to the heat flow data. The default value is False.</p> <code>False</code> <code>smoothing_1st_deriv</code> <code>float</code> <p>Smoothing parameter for the first derivative of the heat flow data. The default value is 1e-9.</p> <code>1e-09</code> <code>smoothing_2nd_deriv</code> <code>float</code> <p>Smoothing parameter for the second derivative of the heat flow data. The default value is 1e-9.</p> <code>1e-09</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>@dataclass\nclass SplineInterpolationParameters:\n    \"\"\"Parameters for spline interpolation of heat flow data.\n\n    Parameters\n    ----------\n\n    apply :\n        Flag indicating whether spline interpolation should be applied to the heat flow data. The default value is False.\n\n    smoothing_1st_deriv :\n        Smoothing parameter for the first derivative of the heat flow data. The default value is 1e-9.\n\n    smoothing_2nd_deriv :\n        Smoothing parameter for the second derivative of the heat flow data. The default value is 1e-9.\n\n    \"\"\"\n\n    apply: bool = False\n    smoothing_1st_deriv: float = 1e-9\n    smoothing_2nd_deriv: float = 1e-9\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.TianCorrectionParameters","title":"<code>TianCorrectionParameters</code>  <code>dataclass</code>","text":"<p>Parameters related to time constants used in Tian's correction method for thermal analysis. The default values are defined in the TianCorrectionParameters class.</p> <p>Parameters:</p> Name Type Description Default <code>tau1</code> <code>int</code> <p>Time constant for the first correction step in Tian's method. The default value is 300.</p> <code>300</code> <code>tau2</code> <code>int</code> <p>Time constant for the second correction step in Tian's method. The default value is 100.</p> <code>100</code> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>@dataclass\nclass TianCorrectionParameters:\n    \"\"\"\n    Parameters related to time constants used in Tian's correction method for thermal analysis. The default values are defined in the TianCorrectionParameters class.\n\n    Parameters\n    ----------\n    tau1 : int\n        Time constant for the first correction step in Tian's method. The default value is 300.\n\n    tau2 : int\n        Time constant for the second correction step in Tian's method. The default value is 100.\n    \"\"\"\n\n    tau1: int = 300\n    tau2: int = 100\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.adaptive_downsample","title":"<code>adaptive_downsample(df, x_col, y_col, processparams)</code>","text":"<p>Adaptively downsample a DataFrame based on the second derivative magnitude.</p> <p>Parameters: - df: pandas DataFrame with columns 'x' and 'y'. - x_col: String for the 'x' values column of the DataFrame. - y_col: String for the 'y' values column of the DataFrame. - num_points: Desired number of points in the downsampled DataFrame. - smoothing_factor: Smoothing factor for the spline interpolation.</p> <p>Returns: - downsampled_df: Downsampled pandas DataFrame.</p> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def adaptive_downsample(\n    df, x_col, y_col, processparams: ProcessingParameters\n):\n    \"\"\"\n    Adaptively downsample a DataFrame based on the second derivative magnitude.\n\n    Parameters:\n    - df: pandas DataFrame with columns 'x' and 'y'.\n    - x_col: String for the 'x' values column of the DataFrame.\n    - y_col: String for the 'y' values column of the DataFrame.\n    - num_points: Desired number of points in the downsampled DataFrame.\n    - smoothing_factor: Smoothing factor for the spline interpolation.\n\n    Returns:\n    - downsampled_df: Downsampled pandas DataFrame.\n    \"\"\"\n\n    if processparams.downsample.section_split:\n        num_points = int(processparams.downsample.num_points / 2)\n\n    #df = df.query(\"time_s &gt; 1800\")\n    x = df[x_col].values\n    y = df[y_col].values\n\n    # print(y)\n    # interpolate the data\n    spl = UnivariateSpline(x, y, s=processparams.downsample.smoothing_factor)\n    new_x = x  # np.linspace(x.min(), x.max(), len(x))\n    # print(new_x)\n    new_y = spl(new_x)\n\n    # Compute the first derivative (gradient)\n    dy_dx = np.gradient(new_y, new_x)\n\n    # Compute the second derivative\n    d2y_dx2 = np.gradient(dy_dx, new_x)\n\n    # Compute the absolute value of the second derivative\n    curvature = np.abs(d2y_dx2)\n    # Avoid division by zero by adding a small constant\n    curvature += 1e-15\n    # Normalize curvature\n    curvature_normalized = curvature / curvature.sum()\n\n    # Create PDF with a baseline to ensure sampling in low-curvature areas\n    baseline_weight = processparams.downsample.baseline_weight\n    pdf = curvature_normalized + baseline_weight / num_points\n    pdf /= pdf.sum()  # Normalize to create a valid PDF\n\n    # Compute CDF\n    cdf = np.cumsum(pdf)\n\n    # plt.plot(x, y)\n    # plt.plot(x, new_y, label=\"interpolated\")\n    # plt.plot(x, y, label=\"raw\")\n    # plt.plot(x, new_y, label=\"interpolated\")\n    # plt.plot(x, dy_dx, label=\"gradient\")\n    # plt.plot(x, d2y_dx2, label=\"second derivative\")\n    # plt.plot(x, curvature, label=\"curvature\")\n    # plt.scatter(new_x, pdf, label=\"pdf\")\n\n    # plt.plot(x, cdf, label=\"cdf\")\n    # plt.legend()\n    # # plt.yscale(\"log\")\n    # plt.show()\n\n    # # # Generate uniformly spaced samples in the interval [0, 1)\n    uniform_samples = np.linspace(0, 1, num_points, endpoint=False)\n\n    # # # Map uniform samples to indices using the inverse CDF\n    indices = np.searchsorted(cdf, uniform_samples)\n    # print(indices)\n    # # # Ensure indices are within valid range\n\n    indices = np.clip(indices, 0, len(df) - 1)\n    # #\n\n    # # Remove duplicates and sort indices\n    indices = np.unique(indices)\n    # indices = np.argsort(pdf)[-num_points:]\n    # print(indices)\n\n    # Subsample the DataFrame at these indices\n    downsampled_df = df.iloc[indices]\n    # name of sample\n    sample_name = df[\"sample_short\"].iloc[0]\n    print(f\"Downsampled {sample_name} to\", len(downsampled_df), \"points\")\n    return downsampled_df\n</code></pre>"},{"location":"reference.html#CaloCem.tacalorimetry.downsample_sections","title":"<code>downsample_sections(df, x_col, y_col, processparams)</code>","text":"<p>Downsample a DataFrame by dividing it into sections and downsampling each section individually.</p> <p>Parameters: - df: pandas DataFrame with columns 'x' and 'y'. - x_col: String for the 'x' values column of the DataFrame. - y_col: String for the 'y' values column of the DataFrame. - num_points: Desired number of points in the downsampled DataFrame. - smoothing_factor: Smoothing factor for the spline interpolation.</p> <p>Returns: - downsampled_df: Downsampled pandas DataFrame.</p> Source code in <code>CaloCem/tacalorimetry.py</code> <pre><code>def downsample_sections(df, x_col, y_col, processparams):\n    \"\"\"\n    Downsample a DataFrame by dividing it into sections and downsampling each section individually.\n\n    Parameters:\n    - df: pandas DataFrame with columns 'x' and 'y'.\n    - x_col: String for the 'x' values column of the DataFrame.\n    - y_col: String for the 'y' values column of the DataFrame.\n    - num_points: Desired number of points in the downsampled DataFrame.\n    - smoothing_factor: Smoothing factor for the spline interpolation.\n\n    Returns:\n    - downsampled_df: Downsampled pandas DataFrame.\n    \"\"\"\n    # Time Split\n    time_split = processparams.downsample.section_split_time_s #1000\n\n    # Split the DataFrame into sections based on the time column\n    df1 = df[df[x_col] &lt; time_split]\n    df2 = df[df[x_col] &gt;= time_split]\n\n    # Downsample each section individually\n    downsampled_df1 = adaptive_downsample(\n        df1, x_col, y_col, processparams\n    )\n    downsampled_df2 = adaptive_downsample(\n        df2, x_col, y_col, processparams\n    )\n\n    # Concatenate the downsampled sections\n    downsampled_df = pd.concat([downsampled_df1, downsampled_df2])\n\n    return downsampled_df\n</code></pre>"},{"location":"tian.html","title":"Tian Correction","text":""},{"location":"tian.html#example-workflow","title":"Example Workflow","text":""},{"location":"tian.html#load-the-data","title":"Load the Data","text":"<p>For fast reactions, e.g., reactions occuring during the first minutes of cement hydration, the thermal inertia of the calorimeter significantly broadens the heat flow signal.  If the characteristic time constants are determined experimentally, a Tian correction can be applied to the heat flow data.</p> <p>First we load the data.</p> <pre><code>from pathlib import Path\n\nimport matplotlib.pyplot as plt\n\nimport CaloCem.tacalorimetry as ta\n\ndatapath = Path(__file__).parent / \"calo_data\"\n\n# load experimental data\ntam = ta.Measurement(\n    folder=datapath,\n    regex=r\".*file1.csv\",\n    show_info=True,\n    auto_clean=False,\n    cold_start=True,\n)\n</code></pre> <p>In the next step, we need to define a few parameters which are necessary for the Tian correction.  Therefore we create a ProcessingParameters object which we call <code>processparams</code> in this case. The <code>processparams</code> object has a number of attributes which we can define. First, we define two time constants <code>tau1</code> and <code>tau2</code>. The numeric value needs to be determined experimentally.</p> <pre><code># Set Proceesing Parameters\nprocessparams = ta.ProcessingParameters()\nprocessparams.time_constants.tau1 = 240\nprocessparams.time_constants.tau2 = 80\nprocessparams.median_filter.apply = True\nprocessparams.median_filter.size = 15\nprocessparams.spline_interpolation.apply = True\nprocessparams.spline_interpolation.smoothing_1st_deriv = 1e-10\nprocessparams.spline_interpolation.smoothing_2nd_deriv = 1e-10\n</code></pre> <p>Next we apply the Tian correction by calling the method <code>apply_tian_correction()</code>. We pass the <code>processparams</code> object defined above to the method.</p> <pre><code># apply tian correction\ntam.apply_tian_correction(\n    processparams=processparams,\n)\n</code></pre> <p>Finally, we can get the Pandas dataframe containing the calorimetric data by calling <code>get_data()</code>. Using the <code>df</code> DataFrame we can plot the calorimetry data using well-known Matplotlib methods.</p> <pre><code>df = tam.get_data()\n\n# plot corrected and uncorrected data\nfig, ax = plt.subplots()\nax.plot(\n    df[\"time_s\"] / 60,\n    df[\"normalized_heat_flow_w_g\"],\n    linestyle=\"--\",\n    label=\"sample\"\n    )\nax.plot(\n    df[\"time_s\"] / 60,\n    df[\"normalized_heat_flow_w_g_tian\"],\n    color=ax.get_lines()[-1].get_color(),\n    label=\"Tian corrected\"\n    )\nax.set_xlim(0, 15)\nax.set_xlabel(\"Time (min)\")\nax.set_ylabel(\"Normalized Heat Flow (W/g)\")\nax.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"tian.html#one-or-two-tau-values","title":"One or Two Tau Values","text":"<p>If only one Tau value is defined, the correction algorithm will only consider this \\(\\tau\\) value and the data will be corrected according to </p> \\[ P(t) = \\varepsilon \\left[ U(t) + \\tau \\frac{dU(t)}{dt} \\right] \\] <p>If two values for \\(\\tau\\) are provided, the data will be corrected considering both values.</p> \\[ P(t) = \\varepsilon \\left[ U(t) + (\\tau_1+\\tau_2) \\frac{dU(t)}{dt} + \\tau_1\\tau_2 \\frac{d^2U}{dt^2} \\right] \\] <p>The actual implementation of the correction algorithm is not based on the voltage \\(U\\) but on the heat flow.  In most cases, the exported data does not contain the raw voltage data but the heat flow data which has been obtained in the instrument software with the experimentally determined value for \\(\\varepsilon\\).</p> <p>Therefore, the second equation reads like</p> \\[ \\dot{Q}_{Tian}(t) =  \\dot{Q}(t) + (\\tau_1+\\tau_2) \\frac{\\dot{Q}}{dt} + \\tau_1\\tau_2 \\frac{d^2\\dot{Q}}{dt^2}  \\] <p>In pratical terms, if only the attribute tau1 is set, only the first derivative of the heat flow will be considered.</p> <pre><code># Set Proceesing Parameters\nprocessparams = ta.ProcessingParameters()\nprocessparams.time_constants.tau1 = 300\n</code></pre> <p>The difference between having one or two tau constants can be seen in the following plot. In general, having two tau constants renders the signal even more narrow.</p> <p></p>"},{"location":"tian.html#smoothing-the-data","title":"Smoothing the Data","text":""},{"location":"tian.html#no-smoothing","title":"No smoothing","text":"<p>It is important to smoothen the data.  Otherwise, small noise in the raw heat flow data will lead to significant noise especially in the second derivative. By default the smoothing is no applied.  Here, we explicitly set the attributes to <code>False</code> and repeat the analysis as shown above. The results demonstrates the noise in the data which originates from tiny fluctuations which result in significant noise in the second derivative.</p> <pre><code># Set Proceesing Parameters\nprocessparams.median_filter.apply = False\nprocessparams.spline_interpolation.apply = False\n</code></pre> <p></p>"},{"location":"tian.html#only-median-filter","title":"Only Median Filter","text":"<p>Here is the result of only applying a median filter with a size of 15.</p> <p><pre><code># Set Proceesing Parameters\nprocessparams.median_filter.apply = True\nprocessparams.spline_interpolation.apply = 15\n</code></pre> </p>"},{"location":"tian.html#only-spline-smoothing","title":"Only Spline Smoothing","text":"<p>Here is the result of only applying a Univariate Spline with smmothing of 1e-10 for both the first and the second derivative. The combination of a median filter and spline smoothing reliably delivers smooth data without introducing artifacts or significant line broadening.</p> <p><pre><code># Set Proceesing Parameters\nprocessparams.median_filter.apply = False\nprocessparams.spline_interpolation.apply = True\nprocessparams.spline_interpolation.smoothing_1st_deriv = 1e-10\nprocessparams.spline_interpolation.smoothing_2nd_deriv = 1e-10\n</code></pre> </p>"}]}